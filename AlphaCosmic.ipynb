{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphaCosmic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfPCzA/CYlnRwxifm11f0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satoruk-icepp/MEG2XEC/blob/master/AlphaCosmic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtlffiOPSg7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install uproot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNis6vBvUYW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import cluster\n",
        "import uproot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJGdIN1SMAKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f128d56-90e5-47c1-ee0e-f06bf1c4352c"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gH1JXWsViYe",
        "colab_type": "code",
        "outputId": "740183f0-5e80-4e51-de1e-0fda10651d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaGsjQI4VIy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = uproot.open(\"/content/drive/My Drive/MEG2CW/alpha351601.root\")\n",
        "tout = file[\"talpha\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCXtwsZFVV2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbb26773-6db2-4913-db37-953b98430f47"
      },
      "source": [
        "PMResponse = tout.array(\"nphoarray\")\n",
        "PosAll = tout.array(\"alphaposall\")\n",
        "alphaid=tout.array(\"alphaid\")\n",
        "qaratio=tout.array(\"qaratio\")\n",
        "indx_wire2 = alphaid//5==2\n",
        "isalpha = np.array((qaratio > 4) & (qaratio<6)).astype(int)\n",
        "print(isalpha)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 ... 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8KZwOkUO3Ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9acab0dc-98a7-42f3-9a77-a1df74b78c0b"
      },
      "source": [
        "PMResponse_train,PMResponse_test = np.split(PMResponse,[3000])\n",
        "isalpha_train,isalpha_test = np.split(isalpha,[3000])\n",
        "print(len(PMResponse_train),len(isalpha_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 1035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAVi_KbhMKZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygzug4LtMQRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "7ceacc16-ab05-43bb-dd2f-f0b0ca973c30"
      },
      "source": [
        "print(model(PMResponse_train.reshape(PMResponse_train.shape[0],-1)))\n",
        "predictions = model(PMResponse_train.reshape(PMResponse_train.shape[0],-1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer dense_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "tf.Tensor(\n",
            "[[9.9959332e-01]\n",
            " [2.0743153e-12]\n",
            " [4.6820436e-13]\n",
            " ...\n",
            " [3.3332739e-04]\n",
            " [1.4444336e-16]\n",
            " [1.0000000e+00]], shape=(3000, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSdgvX11OxJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bce = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC3tP1usMXNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=bce,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyjRwgWkND9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4217c1e-af9c-4217-d4bf-18a38bda1b32"
      },
      "source": [
        "model.fit(PMResponse_train,isalpha_train, epochs=100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3000 samples\n",
            "Epoch 1/100\n",
            "3000/3000 [==============================] - 1s 267us/sample - loss: 0.1566 - accuracy: 0.9430\n",
            "Epoch 2/100\n",
            "3000/3000 [==============================] - 1s 284us/sample - loss: 0.1726 - accuracy: 0.9417\n",
            "Epoch 3/100\n",
            "3000/3000 [==============================] - 1s 272us/sample - loss: 0.1343 - accuracy: 0.9503\n",
            "Epoch 4/100\n",
            "3000/3000 [==============================] - 1s 275us/sample - loss: 0.1035 - accuracy: 0.9580\n",
            "Epoch 5/100\n",
            "3000/3000 [==============================] - 1s 278us/sample - loss: 0.1051 - accuracy: 0.9617\n",
            "Epoch 6/100\n",
            "3000/3000 [==============================] - 1s 266us/sample - loss: 0.1251 - accuracy: 0.9593\n",
            "Epoch 7/100\n",
            "3000/3000 [==============================] - 1s 269us/sample - loss: 0.1980 - accuracy: 0.9420\n",
            "Epoch 8/100\n",
            "3000/3000 [==============================] - 1s 275us/sample - loss: 0.1982 - accuracy: 0.9493\n",
            "Epoch 9/100\n",
            "3000/3000 [==============================] - 1s 272us/sample - loss: 0.2937 - accuracy: 0.9363\n",
            "Epoch 10/100\n",
            "3000/3000 [==============================] - 1s 263us/sample - loss: 0.1844 - accuracy: 0.9470\n",
            "Epoch 11/100\n",
            "3000/3000 [==============================] - 1s 264us/sample - loss: 0.1228 - accuracy: 0.9617\n",
            "Epoch 12/100\n",
            "3000/3000 [==============================] - 1s 255us/sample - loss: 0.3187 - accuracy: 0.9420\n",
            "Epoch 13/100\n",
            "3000/3000 [==============================] - 1s 258us/sample - loss: 0.4969 - accuracy: 0.9217\n",
            "Epoch 14/100\n",
            "3000/3000 [==============================] - 1s 275us/sample - loss: 0.5243 - accuracy: 0.9210\n",
            "Epoch 15/100\n",
            "3000/3000 [==============================] - 1s 263us/sample - loss: 0.1833 - accuracy: 0.9440\n",
            "Epoch 16/100\n",
            "3000/3000 [==============================] - 1s 267us/sample - loss: 0.2509 - accuracy: 0.9473\n",
            "Epoch 17/100\n",
            "3000/3000 [==============================] - 1s 267us/sample - loss: 0.1290 - accuracy: 0.9487\n",
            "Epoch 18/100\n",
            "3000/3000 [==============================] - 1s 290us/sample - loss: 0.1409 - accuracy: 0.9570\n",
            "Epoch 19/100\n",
            "3000/3000 [==============================] - 1s 282us/sample - loss: 0.1046 - accuracy: 0.9630\n",
            "Epoch 20/100\n",
            "3000/3000 [==============================] - 1s 266us/sample - loss: 0.0826 - accuracy: 0.9687\n",
            "Epoch 21/100\n",
            "3000/3000 [==============================] - 1s 276us/sample - loss: 0.0973 - accuracy: 0.9663\n",
            "Epoch 22/100\n",
            "3000/3000 [==============================] - 1s 279us/sample - loss: 0.2525 - accuracy: 0.9417\n",
            "Epoch 23/100\n",
            "3000/3000 [==============================] - 1s 268us/sample - loss: 0.1356 - accuracy: 0.9500\n",
            "Epoch 24/100\n",
            "3000/3000 [==============================] - 1s 274us/sample - loss: 0.3463 - accuracy: 0.9450\n",
            "Epoch 25/100\n",
            "3000/3000 [==============================] - 1s 278us/sample - loss: 0.1646 - accuracy: 0.9530\n",
            "Epoch 26/100\n",
            "3000/3000 [==============================] - 1s 275us/sample - loss: 0.1054 - accuracy: 0.9673\n",
            "Epoch 27/100\n",
            "3000/3000 [==============================] - 1s 273us/sample - loss: 0.1046 - accuracy: 0.9650\n",
            "Epoch 28/100\n",
            "3000/3000 [==============================] - 1s 269us/sample - loss: 0.0923 - accuracy: 0.9637\n",
            "Epoch 29/100\n",
            "3000/3000 [==============================] - 1s 271us/sample - loss: 0.1209 - accuracy: 0.9673\n",
            "Epoch 30/100\n",
            "3000/3000 [==============================] - 1s 262us/sample - loss: 0.0844 - accuracy: 0.9707\n",
            "Epoch 31/100\n",
            "3000/3000 [==============================] - 1s 260us/sample - loss: 0.0745 - accuracy: 0.9717\n",
            "Epoch 32/100\n",
            "3000/3000 [==============================] - 1s 269us/sample - loss: 0.0705 - accuracy: 0.9750\n",
            "Epoch 33/100\n",
            "3000/3000 [==============================] - 1s 248us/sample - loss: 0.0792 - accuracy: 0.9753\n",
            "Epoch 34/100\n",
            "3000/3000 [==============================] - 1s 257us/sample - loss: 0.1025 - accuracy: 0.9697\n",
            "Epoch 35/100\n",
            "3000/3000 [==============================] - 1s 264us/sample - loss: 0.0978 - accuracy: 0.9663\n",
            "Epoch 36/100\n",
            "3000/3000 [==============================] - 1s 269us/sample - loss: 0.1022 - accuracy: 0.9643\n",
            "Epoch 37/100\n",
            "3000/3000 [==============================] - 1s 273us/sample - loss: 0.0724 - accuracy: 0.9723\n",
            "Epoch 38/100\n",
            "3000/3000 [==============================] - 1s 264us/sample - loss: 0.0914 - accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "3000/3000 [==============================] - 1s 276us/sample - loss: 0.0769 - accuracy: 0.9743\n",
            "Epoch 40/100\n",
            "3000/3000 [==============================] - 1s 277us/sample - loss: 0.1233 - accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "3000/3000 [==============================] - 1s 272us/sample - loss: 0.1074 - accuracy: 0.9663\n",
            "Epoch 42/100\n",
            "3000/3000 [==============================] - 1s 275us/sample - loss: 0.0735 - accuracy: 0.9717\n",
            "Epoch 43/100\n",
            "3000/3000 [==============================] - 1s 269us/sample - loss: 0.0894 - accuracy: 0.9690\n",
            "Epoch 44/100\n",
            "3000/3000 [==============================] - 1s 277us/sample - loss: 0.0774 - accuracy: 0.9747\n",
            "Epoch 45/100\n",
            "3000/3000 [==============================] - 1s 280us/sample - loss: 0.1327 - accuracy: 0.9673\n",
            "Epoch 46/100\n",
            "3000/3000 [==============================] - 1s 270us/sample - loss: 0.1907 - accuracy: 0.9527\n",
            "Epoch 47/100\n",
            "3000/3000 [==============================] - 1s 278us/sample - loss: 0.1443 - accuracy: 0.9633\n",
            "Epoch 48/100\n",
            "3000/3000 [==============================] - 1s 276us/sample - loss: 0.5420 - accuracy: 0.9277\n",
            "Epoch 49/100\n",
            "3000/3000 [==============================] - 1s 276us/sample - loss: 0.1681 - accuracy: 0.9480\n",
            "Epoch 50/100\n",
            "3000/3000 [==============================] - 1s 275us/sample - loss: 0.1205 - accuracy: 0.9610\n",
            "Epoch 51/100\n",
            "3000/3000 [==============================] - 1s 264us/sample - loss: 0.1100 - accuracy: 0.9617\n",
            "Epoch 52/100\n",
            "3000/3000 [==============================] - 1s 265us/sample - loss: 0.3250 - accuracy: 0.9627\n",
            "Epoch 53/100\n",
            "3000/3000 [==============================] - 1s 260us/sample - loss: 0.1250 - accuracy: 0.9610\n",
            "Epoch 54/100\n",
            "3000/3000 [==============================] - 1s 267us/sample - loss: 0.1403 - accuracy: 0.9647\n",
            "Epoch 55/100\n",
            "3000/3000 [==============================] - 1s 265us/sample - loss: 0.0766 - accuracy: 0.9757\n",
            "Epoch 56/100\n",
            "3000/3000 [==============================] - 1s 271us/sample - loss: 0.1875 - accuracy: 0.9503\n",
            "Epoch 57/100\n",
            "3000/3000 [==============================] - 1s 266us/sample - loss: 0.1062 - accuracy: 0.9640\n",
            "Epoch 58/100\n",
            "3000/3000 [==============================] - 1s 273us/sample - loss: 0.1655 - accuracy: 0.9533\n",
            "Epoch 59/100\n",
            "3000/3000 [==============================] - 1s 274us/sample - loss: 0.1205 - accuracy: 0.9607\n",
            "Epoch 60/100\n",
            "3000/3000 [==============================] - 1s 274us/sample - loss: 0.2385 - accuracy: 0.9647\n",
            "Epoch 61/100\n",
            "3000/3000 [==============================] - 1s 274us/sample - loss: 0.2198 - accuracy: 0.9613\n",
            "Epoch 62/100\n",
            "3000/3000 [==============================] - 1s 268us/sample - loss: 0.1353 - accuracy: 0.9643\n",
            "Epoch 63/100\n",
            "3000/3000 [==============================] - 1s 270us/sample - loss: 0.1034 - accuracy: 0.9730\n",
            "Epoch 64/100\n",
            "3000/3000 [==============================] - 1s 259us/sample - loss: 0.0642 - accuracy: 0.9773\n",
            "Epoch 65/100\n",
            "3000/3000 [==============================] - 1s 267us/sample - loss: 0.0593 - accuracy: 0.9797\n",
            "Epoch 66/100\n",
            "3000/3000 [==============================] - 1s 271us/sample - loss: 0.0689 - accuracy: 0.9760\n",
            "Epoch 67/100\n",
            "3000/3000 [==============================] - 1s 263us/sample - loss: 0.0708 - accuracy: 0.9737\n",
            "Epoch 68/100\n",
            "3000/3000 [==============================] - 1s 278us/sample - loss: 0.0598 - accuracy: 0.9787\n",
            "Epoch 69/100\n",
            "3000/3000 [==============================] - 1s 273us/sample - loss: 0.0565 - accuracy: 0.9797\n",
            "Epoch 70/100\n",
            "3000/3000 [==============================] - 1s 289us/sample - loss: 0.0460 - accuracy: 0.9827\n",
            "Epoch 71/100\n",
            "3000/3000 [==============================] - 1s 277us/sample - loss: 0.0682 - accuracy: 0.9777\n",
            "Epoch 72/100\n",
            "3000/3000 [==============================] - 1s 264us/sample - loss: 0.0698 - accuracy: 0.9777\n",
            "Epoch 73/100\n",
            "3000/3000 [==============================] - 1s 271us/sample - loss: 0.0589 - accuracy: 0.9797\n",
            "Epoch 74/100\n",
            "3000/3000 [==============================] - 1s 268us/sample - loss: 0.0499 - accuracy: 0.9807\n",
            "Epoch 75/100\n",
            "3000/3000 [==============================] - 1s 268us/sample - loss: 0.0537 - accuracy: 0.9817\n",
            "Epoch 76/100\n",
            "3000/3000 [==============================] - 1s 270us/sample - loss: 0.0537 - accuracy: 0.9820\n",
            "Epoch 77/100\n",
            "3000/3000 [==============================] - 1s 278us/sample - loss: 0.0614 - accuracy: 0.9793\n",
            "Epoch 78/100\n",
            "3000/3000 [==============================] - 1s 270us/sample - loss: 0.0649 - accuracy: 0.9783\n",
            "Epoch 79/100\n",
            "3000/3000 [==============================] - 1s 274us/sample - loss: 0.0584 - accuracy: 0.9803\n",
            "Epoch 80/100\n",
            "3000/3000 [==============================] - 1s 268us/sample - loss: 0.0800 - accuracy: 0.9730\n",
            "Epoch 81/100\n",
            "3000/3000 [==============================] - 1s 268us/sample - loss: 0.0601 - accuracy: 0.9807\n",
            "Epoch 82/100\n",
            "3000/3000 [==============================] - 1s 271us/sample - loss: 0.0685 - accuracy: 0.9773\n",
            "Epoch 83/100\n",
            "3000/3000 [==============================] - 1s 280us/sample - loss: 0.0712 - accuracy: 0.9773\n",
            "Epoch 84/100\n",
            "3000/3000 [==============================] - 1s 272us/sample - loss: 0.0639 - accuracy: 0.9770\n",
            "Epoch 85/100\n",
            "3000/3000 [==============================] - 1s 265us/sample - loss: 0.0659 - accuracy: 0.9777\n",
            "Epoch 86/100\n",
            "3000/3000 [==============================] - 1s 252us/sample - loss: 0.0656 - accuracy: 0.9783\n",
            "Epoch 87/100\n",
            "3000/3000 [==============================] - 1s 261us/sample - loss: 0.0975 - accuracy: 0.9753\n",
            "Epoch 88/100\n",
            "3000/3000 [==============================] - 1s 265us/sample - loss: 0.0609 - accuracy: 0.9757\n",
            "Epoch 89/100\n",
            "3000/3000 [==============================] - 1s 265us/sample - loss: 0.0826 - accuracy: 0.9700\n",
            "Epoch 90/100\n",
            "3000/3000 [==============================] - 1s 261us/sample - loss: 0.0801 - accuracy: 0.9783\n",
            "Epoch 91/100\n",
            "3000/3000 [==============================] - 1s 257us/sample - loss: 0.0787 - accuracy: 0.9763\n",
            "Epoch 92/100\n",
            "3000/3000 [==============================] - 1s 260us/sample - loss: 0.0780 - accuracy: 0.9723\n",
            "Epoch 93/100\n",
            "3000/3000 [==============================] - 1s 271us/sample - loss: 0.0927 - accuracy: 0.9750\n",
            "Epoch 94/100\n",
            "3000/3000 [==============================] - 1s 272us/sample - loss: 0.0706 - accuracy: 0.9737\n",
            "Epoch 95/100\n",
            "3000/3000 [==============================] - 1s 272us/sample - loss: 0.0584 - accuracy: 0.9797\n",
            "Epoch 96/100\n",
            "3000/3000 [==============================] - 1s 278us/sample - loss: 0.0634 - accuracy: 0.9770\n",
            "Epoch 97/100\n",
            "3000/3000 [==============================] - 1s 284us/sample - loss: 0.0781 - accuracy: 0.9740\n",
            "Epoch 98/100\n",
            "3000/3000 [==============================] - 1s 267us/sample - loss: 0.0466 - accuracy: 0.9843\n",
            "Epoch 99/100\n",
            "3000/3000 [==============================] - 1s 273us/sample - loss: 0.0555 - accuracy: 0.9793\n",
            "Epoch 100/100\n",
            "3000/3000 [==============================] - 1s 270us/sample - loss: 0.0642 - accuracy: 0.9793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f61d1b81cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC5RyDgYPYP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "b78aa2dd-e26e-4afb-ce56-356bdf5291f7"
      },
      "source": [
        "isalpha_pred_test = model(PMResponse_test.reshape(PMResponse_test.shape[0],-1))\n",
        "plt.hist(isalpha_pred_test[isalpha_test<0.5].numpy().reshape(-1),color='b',histtype='step')\n",
        "plt.hist(isalpha_pred_test[isalpha_test>0.5].numpy().reshape(-1),color='r',histtype='step')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 24.,   3.,   7.,   2.,   5.,   4.,   3.,   8.,  25., 456.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],\n",
              "       dtype=float32),\n",
              " <a list of 1 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XmSxBmqXhL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PMResponse = PMResponse[indx_wire2]\n",
        "PosAll = PosAll[indx_wire2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtH43JLPwb-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(qaratio,range=(0,10),bins=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRoroErVxXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(alphaid[indx_wire2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NupKCcVHab7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(PosAll[:,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewoXAjAlLeY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJwH0oR1cwwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install optuna\n",
        "# import optuna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXRy25vV0hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def objective():\n",
        "#     clusterer = cluster.DBSCAN(eps=1000,min_samples=4)\n",
        "#     y_pred = clusterer.fit_predict(PMResponse)\n",
        "#     nc = len(np.unique(y_pred))\n",
        "#     return nc\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=100)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrTCA6kLeyrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyNqVbmEWIoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_array = []\n",
        "eps_array = []\n",
        "# for i in range(10):\n",
        "#     # eps = (i+1)*200+1000\n",
        "#     eps\n",
        "eps = 2\n",
        "clusterer = cluster.DBSCAN(eps=eps,min_samples=5)\n",
        "# clusterer = cluster.KMeans(n_clusters=5)\n",
        "# y_pred = clusterer.fit_predict(PMResponse)\n",
        "y_pred = clusterer.fit_predict(PosAll)\n",
        "# if len(np.unique(y_pred))<2: continue\n",
        "# score = metrics.silhouette_score(PMResponse,y_pred)\n",
        "score = metrics.silhouette_score(PosAll,y_pred)\n",
        "print(score)\n",
        "score_array.append(score)\n",
        "eps_array.append(eps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v84tth4KgGuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(eps_array,score_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZvwoFLoe0RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(metrics.silhouette_score(PMResponse,y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbHOgT3xWgn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU5ar5NEWkBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(PosAll[y_pred==4,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU3cR4o6iyoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}