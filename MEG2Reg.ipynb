{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MEG2Reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satoruk-icepp/MEG2XEC/blob/master/MEG2Reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjhoI0mEv7eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch\n",
        "# alist = [0,0,1,1,1,0]\n",
        "# alist = torch.Tensor(alist)\n",
        "# alist = alist.split(3)[0]\n",
        "# alist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdCOQWBz1SuB",
        "colab_type": "code",
        "outputId": "77574d29-6c7e-4259-94c1-17d30ec906d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkZmERLgEDe2",
        "colab_type": "code",
        "outputId": "775dc785-1bd3-4ee3-f17c-e7ab87c79982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile .comet.config\n",
        "[comet]\n",
        "api_key=mIel5ZAPOioTs0Cij75dSSQXs\n",
        "logging_file = /tmp/comet.log\n",
        "logging_file_level = info\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting .comet.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wNHxlzNEGOJ",
        "colab_type": "code",
        "outputId": "0952f6f0-5216-49e2-ae74-826a2b02a632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install skorch comet_ml"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (2.0.17)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.21.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: comet-git-pure>=0.19.11 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.19.13)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.21.0)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.56.0)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (2019.9.11)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.8)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVitehXbEJdo",
        "colab_type": "code",
        "outputId": "65fbd34e-1d1f-43f5-f38b-58fb76ccb740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from comet_ml import Experiment\n",
        "experiment = Experiment(project_name=\"CWreg\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/satoruk-icepp/cwreg/89ef3e11556f4b0baa2b5d6170034c72\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTfFz2Z81Tz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from scipy.optimize import curve_fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opEPrKJbzG8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params={'batch_size' : 200,\n",
        "        'learning_rate':0.001,\n",
        "        'dropout_conv':0.0,\n",
        "        'dropout_fc':0.2,\n",
        "        'optim':\"Adam\",\n",
        "        'weight_decay':1e-05,\n",
        "        'Nresblock':5,\n",
        "        'Wthreshold':np.log(0.2+1e-02)/2.5+1,\n",
        "        'weightstd':0.0001,\n",
        "        'Nlayer':32,\n",
        "        'Nfc':2\n",
        "}\n",
        "experiment.log_parameters(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUcYFNGQCMT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYAOaiY1nWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout.csv')\n",
        "# names=['E','U','V','W','PT','PP','UR','VR','WR']\n",
        "# PMnames = ['PM%d'%(i) for i in range(4760)]\n",
        "# names = names+PMnames\n",
        "# csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout_norm.csv',names=names)\n",
        "csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout_norm.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o880tO_j2LsK",
        "colab_type": "code",
        "outputId": "1f0a2276-0b50-4406-dbf4-018ba51fbe38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "print(csv_data.shape)\n",
        "print(params['Wthreshold'])\n",
        "csv_data = csv_data[csv_data['w']<params['Wthreshold']]\n",
        "csv_data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(53208, 4770)\n",
            "0.3757409006941327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Energy</th>\n",
              "      <th>u</th>\n",
              "      <th>v</th>\n",
              "      <th>w</th>\n",
              "      <th>ptheta</th>\n",
              "      <th>pphi</th>\n",
              "      <th>urec</th>\n",
              "      <th>vrec</th>\n",
              "      <th>wrec</th>\n",
              "      <th>PM_0</th>\n",
              "      <th>PM_1</th>\n",
              "      <th>PM_2</th>\n",
              "      <th>PM_3</th>\n",
              "      <th>PM_4</th>\n",
              "      <th>PM_5</th>\n",
              "      <th>PM_6</th>\n",
              "      <th>PM_7</th>\n",
              "      <th>PM_8</th>\n",
              "      <th>PM_9</th>\n",
              "      <th>PM_10</th>\n",
              "      <th>PM_11</th>\n",
              "      <th>PM_12</th>\n",
              "      <th>PM_13</th>\n",
              "      <th>PM_14</th>\n",
              "      <th>PM_15</th>\n",
              "      <th>PM_16</th>\n",
              "      <th>PM_17</th>\n",
              "      <th>PM_18</th>\n",
              "      <th>PM_19</th>\n",
              "      <th>PM_20</th>\n",
              "      <th>PM_21</th>\n",
              "      <th>PM_22</th>\n",
              "      <th>PM_23</th>\n",
              "      <th>PM_24</th>\n",
              "      <th>PM_25</th>\n",
              "      <th>PM_26</th>\n",
              "      <th>PM_27</th>\n",
              "      <th>PM_28</th>\n",
              "      <th>PM_29</th>\n",
              "      <th>PM_30</th>\n",
              "      <th>...</th>\n",
              "      <th>PM_4721</th>\n",
              "      <th>PM_4722</th>\n",
              "      <th>PM_4723</th>\n",
              "      <th>PM_4724</th>\n",
              "      <th>PM_4725</th>\n",
              "      <th>PM_4726</th>\n",
              "      <th>PM_4727</th>\n",
              "      <th>PM_4728</th>\n",
              "      <th>PM_4729</th>\n",
              "      <th>PM_4730</th>\n",
              "      <th>PM_4731</th>\n",
              "      <th>PM_4732</th>\n",
              "      <th>PM_4733</th>\n",
              "      <th>PM_4734</th>\n",
              "      <th>PM_4735</th>\n",
              "      <th>PM_4736</th>\n",
              "      <th>PM_4737</th>\n",
              "      <th>PM_4738</th>\n",
              "      <th>PM_4739</th>\n",
              "      <th>PM_4740</th>\n",
              "      <th>PM_4741</th>\n",
              "      <th>PM_4742</th>\n",
              "      <th>PM_4743</th>\n",
              "      <th>PM_4744</th>\n",
              "      <th>PM_4745</th>\n",
              "      <th>PM_4746</th>\n",
              "      <th>PM_4747</th>\n",
              "      <th>PM_4748</th>\n",
              "      <th>PM_4749</th>\n",
              "      <th>PM_4750</th>\n",
              "      <th>PM_4751</th>\n",
              "      <th>PM_4752</th>\n",
              "      <th>PM_4753</th>\n",
              "      <th>PM_4754</th>\n",
              "      <th>PM_4755</th>\n",
              "      <th>PM_4756</th>\n",
              "      <th>PM_4757</th>\n",
              "      <th>PM_4758</th>\n",
              "      <th>PM_4759</th>\n",
              "      <th>flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.154239</td>\n",
              "      <td>-0.550071</td>\n",
              "      <td>0.363598</td>\n",
              "      <td>-0.479323</td>\n",
              "      <td>0.412274</td>\n",
              "      <td>-0.285022</td>\n",
              "      <td>-0.551514</td>\n",
              "      <td>0.366742</td>\n",
              "      <td>-0.548834</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.244407</td>\n",
              "      <td>0.660570</td>\n",
              "      <td>0.186308</td>\n",
              "      <td>0.177176</td>\n",
              "      <td>-0.517817</td>\n",
              "      <td>-0.247431</td>\n",
              "      <td>0.654285</td>\n",
              "      <td>0.209709</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.164686</td>\n",
              "      <td>-0.074347</td>\n",
              "      <td>-0.520796</td>\n",
              "      <td>-0.057217</td>\n",
              "      <td>0.056129</td>\n",
              "      <td>0.408249</td>\n",
              "      <td>-0.089807</td>\n",
              "      <td>-0.523477</td>\n",
              "      <td>-0.035691</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.166454</td>\n",
              "      <td>-0.057256</td>\n",
              "      <td>0.117176</td>\n",
              "      <td>-0.841152</td>\n",
              "      <td>0.044864</td>\n",
              "      <td>-0.091854</td>\n",
              "      <td>-0.045502</td>\n",
              "      <td>0.112746</td>\n",
              "      <td>-0.697976</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0074</td>\n",
              "      <td>0.0069</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.547770</td>\n",
              "      <td>-0.514685</td>\n",
              "      <td>-0.163782</td>\n",
              "      <td>-0.403832</td>\n",
              "      <td>0.403459</td>\n",
              "      <td>0.550551</td>\n",
              "      <td>-0.514413</td>\n",
              "      <td>-0.080183</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53202</th>\n",
              "      <td>0.173719</td>\n",
              "      <td>-0.048726</td>\n",
              "      <td>0.153198</td>\n",
              "      <td>-0.553207</td>\n",
              "      <td>0.037937</td>\n",
              "      <td>-0.120091</td>\n",
              "      <td>-0.031658</td>\n",
              "      <td>0.155007</td>\n",
              "      <td>-0.353890</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0074</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53203</th>\n",
              "      <td>0.165990</td>\n",
              "      <td>0.303824</td>\n",
              "      <td>0.508643</td>\n",
              "      <td>0.122589</td>\n",
              "      <td>-0.221882</td>\n",
              "      <td>-0.398722</td>\n",
              "      <td>0.302143</td>\n",
              "      <td>0.510321</td>\n",
              "      <td>0.138353</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53204</th>\n",
              "      <td>0.150154</td>\n",
              "      <td>-0.148314</td>\n",
              "      <td>0.607137</td>\n",
              "      <td>0.140384</td>\n",
              "      <td>0.108837</td>\n",
              "      <td>-0.475931</td>\n",
              "      <td>-0.164124</td>\n",
              "      <td>0.604553</td>\n",
              "      <td>0.155343</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53205</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.192877</td>\n",
              "      <td>0.433645</td>\n",
              "      <td>-0.114305</td>\n",
              "      <td>0.145891</td>\n",
              "      <td>-0.339932</td>\n",
              "      <td>-0.186460</td>\n",
              "      <td>0.427623</td>\n",
              "      <td>-0.133480</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53207</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.593930</td>\n",
              "      <td>-0.159663</td>\n",
              "      <td>0.247205</td>\n",
              "      <td>0.412961</td>\n",
              "      <td>0.125159</td>\n",
              "      <td>-0.612349</td>\n",
              "      <td>-0.162239</td>\n",
              "      <td>0.440602</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39226 rows  4770 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Energy         u         v         w  ...  PM_4757  PM_4758  PM_4759  flag\n",
              "1      0.154239 -0.550071  0.363598 -0.479323  ...   0.0010   0.0016   0.0011     0\n",
              "4      0.176200 -0.244407  0.660570  0.186308  ...   0.0005   0.0004   0.0005     0\n",
              "5      0.164686 -0.074347 -0.520796 -0.057217  ...   0.0023   0.0018   0.0031     0\n",
              "6      0.166454 -0.057256  0.117176 -0.841152  ...   0.0048   0.0045   0.0057     0\n",
              "7      0.176200  0.547770 -0.514685 -0.163782  ...   0.0030   0.0017   0.0023     0\n",
              "...         ...       ...       ...       ...  ...      ...      ...      ...   ...\n",
              "53202  0.173719 -0.048726  0.153198 -0.553207  ...   0.0039   0.0053   0.0039     0\n",
              "53203  0.165990  0.303824  0.508643  0.122589  ...   0.0011   0.0017   0.0013     0\n",
              "53204  0.150154 -0.148314  0.607137  0.140384  ...   0.0010   0.0004   0.0006     0\n",
              "53205  0.176200 -0.192877  0.433645 -0.114305  ...   0.0013   0.0023   0.0023     0\n",
              "53207  0.176200 -0.593930 -0.159663  0.247205  ...   0.0036   0.0118   0.0046     0\n",
              "\n",
              "[39226 rows x 4770 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E8pdRH32YPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_data_numpy = csv_data.to_numpy()\n",
        "del csv_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCD9VPG5UBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy  = csv_data_numpy[:,0]\n",
        "UVW     = csv_data_numpy[:,1:4]\n",
        "DIR     = csv_data_numpy[:,4:6]\n",
        "UVWREC  = csv_data_numpy[:,6:9]\n",
        "PMResponse = csv_data_numpy[:,9:4101]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUpGkRmH5xUl",
        "colab_type": "code",
        "outputId": "b7ea99c8-ba0d-4003-9ed6-c97e8237ba5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# PMResponse = csv_data_numpy[:,6:4766]\n",
        "\n",
        "# PMRMS = np.sqrt(np.mean(np.square(PMResponse[:]),axis=1)).reshape(-1,1)\n",
        "# print(PMRMS.shape)\n",
        "Energy     = Energy.reshape(-1,1)\n",
        "PMResponse = PMResponse.reshape(-1,93,44)\n",
        "# PMU = np.average(np.sum(PMResponse[:],axis=1),axis=1,weights=range(0,44)).reshape(-1,1)\n",
        "# PMV = np.average(np.sum(PMResponse[:],axis=2),axis=1,weights=range(0,93)).reshape(-1,1)\n",
        "indx_U = np.arange(-21.5,22.5)\n",
        "indx_V = np.arange(-46,47)\n",
        "# for i in np.arange(-21.5,22.5):\n",
        "PMU = (np.dot(np.sum(PMResponse[:],axis=1),indx_U)/np.sum(np.sum(PMResponse[:],axis=1),axis=1))/22\n",
        "PMV = (np.dot(np.sum(PMResponse[:],axis=2),indx_V)/np.sum(np.sum(PMResponse[:],axis=2),axis=1))/46.5\n",
        "PMU = PMU.reshape(-1,1)\n",
        "PMV = PMV.reshape(-1,1)\n",
        "PMURMS = np.sqrt(np.mean(np.square(np.sum(PMResponse[:],axis=1)),axis=1)).reshape(-1,1)\n",
        "PMVRMS = np.sqrt(np.mean(np.square(np.sum(PMResponse[:],axis=2)),axis=1)).reshape(-1,1)\n",
        "# PMURMS = np.zeros(PMU.shape[0])\n",
        "# for i in range(0,44):\n",
        "    # print(np.sum(PMResponse[:],axis=1)[:,i])\n",
        "# i=0\n",
        "# PMURMS = np.sum(PMResponse[:],axis=1)[:,i]*(PMU*22 - indx_U[i])**2\n",
        "# print(PMURMS)\n",
        "# PMURMS = np.sqrt(np.mean(PMResponse[:]*np.square(,axis=1),axis=1,weights=range(0,44)).reshape(-1,1)\n",
        "\n",
        "# PMURMS = np.sum(np.square(PMU-))\n",
        "# PMRMS = PMRMS.reshape(-1,1)\n",
        "# PMU = PMU.reshape(-1,1)\n",
        "# PMU = PMU.reshape(-1,1)\n",
        "# np.sum(PMResponse[:],axis=1).shape\n",
        "# PMU\n",
        "# plt.hist2d(UVW[:,0],PMU.reshape(-1))\n",
        "plt.hist2d(UVW[:,2],PMURMS.reshape(-1),bins=[40,40])\n",
        "ADD = np.concatenate((PMU,PMV,PMURMS,PMVRMS),axis=1)\n",
        "# print(ADD)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+vfed0UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr8qby97dTE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del csv_data_numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H9hIG-16eUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.scatter(Energy,ZRP[:,2])\n",
        "# plt.scatter(ZRP[:,0],ZRP[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amCUPP4zB_Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.hist(ZRP[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bh8c8cKCTDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_func(x,lb,C):\n",
        "    return C*np.exp(-x/lb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnOKmEKmChPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # data = np.random.exponential(scale=2.0, size=100000)\n",
        "# # data2 = np.random.normal(loc=3.0, scale=0.3, size=15000)\n",
        "# bins = np.linspace(0, 1, 60)\n",
        "# data_entries_1, bins_1 = np.histogram(ZRP[:,1], bins=bins)\n",
        "# # data_entries_2, bins_2 = np.histogram(data2, bins=bins)\n",
        "# binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n",
        "# popt, pcov = curve_fit(fit_func, xdata=binscenters, ydata=data_entries_1, p0=[0.3, 2000.])\n",
        "# print(popt)\n",
        "# xspace = np.linspace(0, 1, 100000)\n",
        "# plt.bar(binscenters, data_entries_1, width=bins[1] - bins[0], color='navy', label=r'Histogram entries')\n",
        "# plt.plot(xspace, fit_func(xspace, *popt), color='darkorange', linewidth=2.5, label=r'Fitted function')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7NNG8bb66hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.imshow(PMResponse[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBCtjK5S_ro4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PMResponse = PMResponse.reshape(-1,4092)\n",
        "# PMResponse = PMResponse/PMResponseScale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9LjR3Nq_nUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.hist(PMResponse.max(axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU_4kZYGChlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy        = torch.tensor(Energy).float()\n",
        "UVW           = torch.tensor(UVW).float()\n",
        "DIR           = torch.tensor(DIR).float()\n",
        "ADD           = torch.tensor(ADD).float()\n",
        "UVWREC        = torch.tensor(UVWREC).float()\n",
        "PMResponse    = torch.tensor(PMResponse).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGwOp69_eMd",
        "colab_type": "code",
        "outputId": "8212d0f0-0d0a-47d0-b8d5-e91eac25392f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.utils.data.dataset import Subset\n",
        "BATCH_SIZE = params[\"batch_size\"]\n",
        "calo_dataset    = utils.TensorDataset(Energy,UVW,UVWREC,DIR,ADD,PMResponse)\n",
        "data_size =  len(calo_dataset)\n",
        "full_size = int(data_size/1000)*1000\n",
        "print(data_size)\n",
        "train_dataset = Subset(calo_dataset,list(range(0,int(0.8*full_size))))\n",
        "val_dataset = Subset(calo_dataset,list(range(int(0.8*full_size),full_size)))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                              batch_size=BATCH_SIZE, \n",
        "                                              pin_memory=True, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                              batch_size=len(val_dataset), \n",
        "                                              pin_memory=True, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xa9GjOIattu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d, nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TiG4ON2EEi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(ResidualBlock, self).__init__()        \n",
        "        self.conv1 = nn.Conv2d(input_size,input_size,3,padding=1)\n",
        "        self.conv2 = nn.Conv2d(input_size,input_size,3,padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)        \n",
        "        self.activation = nn.LeakyReLU(0.0)\n",
        "    def forward(self,xraw):\n",
        "        x = self.activation(self.bn1(self.conv1(xraw)))\n",
        "        x = self.activation(self.bn2(self.conv2(x))+xraw)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0QEoDZM8OE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Regressor(nn.Module):\n",
        "    def __init__(self, dropout_conv =0.0,dropout_fc=0.0,Nresblock=0,Nlayer=32,Nfc=4):\n",
        "        super(Regressor, self).__init__()\n",
        "        # self.fc1 = nn.Linear(4092,256)\n",
        "        self.conv1 = nn.Conv2d(1, Nlayer, kernel_size=(10,10), stride = 5, padding = (1,3))#(93+6,44+6)->24,12\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            self.conv1.out_channels, \n",
        "            self.conv1.out_channels*2, \n",
        "            kernel_size=(4, 3), \n",
        "            stride=2\n",
        "            # ,padding = (1,1)\n",
        "        )#24*12->12*6\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            self.conv2.out_channels, \n",
        "            self.conv2.out_channels*2, \n",
        "            3\n",
        "            # ,stride=2\n",
        "        )#12*6->6*3\n",
        "        self.rb = ResidualBlock(self.conv1.out_channels)\n",
        "        self.fcstart = nn.Linear(self.conv3.out_channels*12+7,256)\n",
        "        self.Nfc = Nfc\n",
        "        self.fc=[nn.Linear(self.fcstart.out_features//2**i,self.fcstart.out_features//2**(i+1)).to(device) for i in range(self.Nfc)]\n",
        "        # # Nnodes = self.fcstart.out_features\n",
        "        # for i in range(Nfc):\n",
        "        #     fctmp = \n",
        "        #     self.fc.append(fctmp)\n",
        "        \n",
        "        # self.fc3 = nn.Linear(self.fc2.out_features,self.fc2.out_features//2)\n",
        "        # self.fc4 = nn.Linear(self.fc3.out_features,self.fc3.out_features//2)\n",
        "        self.fcend = nn.Linear(self.fcstart.out_features//2**(Nfc),3)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(self.conv3.out_channels)\n",
        "        self.dropout1 = nn.Dropout(dropout_conv)\n",
        "        self.dropoutfc = nn.Dropout(dropout_fc)\n",
        "        self.Nresblock = Nresblock\n",
        "        \n",
        "    def forward(self, x, uvw_rec):\n",
        "        # x = F.relu(self.dropout_fc1(self.fc1(x)))\n",
        "        x = x.view(x.shape[0],1,-1)\n",
        "        \n",
        "        # x_mppc,x_pmt = x.split(4092)\n",
        "        x = x.view(x.shape[0],1,93,44)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        for i in range(self.Nresblock):\n",
        "            x = self.dropout1(self.rb(x))\n",
        "        x = F.relu(self.dropout1(self.bn2(self.conv2(x))))\n",
        "        x = F.relu(self.dropout1(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        x = x.view(x.shape[0],self.conv3.out_channels*12)\n",
        "        x = torch.cat([x,uvw_rec],dim=1)\n",
        "        x = F.relu(self.fcstart(x))\n",
        "        for i in range(self.Nfc):\n",
        "            x = F.relu(self.dropoutfc(self.fc[i](x)))\n",
        "        # x = F.relu(self.dropoutfc(self.fc3(x)))\n",
        "        # x = F.relu(self.dropoutfc(self.fc4(x)))\n",
        "        x = self.fcend(x)\n",
        "        return torch.tanh(x)\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHrkeby_Xde",
        "colab_type": "code",
        "outputId": "8ca4a4e5-2e6b-409f-b1e6-dd27c0fafc4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "model = Regressor(\n",
        "    params[\"dropout_conv\"],\n",
        "    params[\"dropout_fc\"],\n",
        "    params[\"Nresblock\"],\n",
        "    params['Nlayer'],\n",
        "    params['Nfc']\n",
        "    ).to(device)\n",
        "print(model)\n",
        "# model.weight_init(mean=0.0, std=params['weightstd'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressor(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(10, 10), stride=(5, 5), padding=(1, 3))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(4, 3), stride=(2, 2))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (rb): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (activation): LeakyReLU(negative_slope=0.0)\n",
            "  )\n",
            "  (fcstart): Linear(in_features=1543, out_features=256, bias=True)\n",
            "  (fcend): Linear(in_features=64, out_features=3, bias=True)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.0, inplace=False)\n",
            "  (dropoutfc): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsT0tiw2Cq0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning_rate = 0.001\n",
        "# opt = optim.Adam(regressor.parameters(), lr=learning_rate)\n",
        "opt = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV8n2CduHEg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy_mean, UVW_mean, DIR_mean = Energy.mean(dim=0).to(device), UVW.mean(dim=0).to(device), DIR.mean(dim=0).to(device)\n",
        "EZRP_mean = torch.cat([Energy_mean, UVW_mean]).to(device)\n",
        "UVWDIR_mean = torch.cat([UVW_mean, DIR_mean]).to(device)\n",
        "def metric_relative_mse(y_pred,y_true):\n",
        "    y_true_mean = y_true.mean(dim=0)\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - EZRP_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVW_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    return (((y_true[:,2] - y_pred[:,2]).pow(2).mean(dim=0) / (y_true[:,2] - UVW_mean[2]).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVWDIR_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - Energy_mean).pow(2).mean(dim=0)).sum()).sqrt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue2O6r-2D1zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_fn = torch.nn.SmoothL1Loss().to(device)\n",
        "loss_fn = torch.nn.L1Loss().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmJdk_POC9Mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(epochs=100):\n",
        "    # iterating over epochs...\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        first = True\n",
        "        for Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b in train_dataloader:\n",
        "        # moving them to device(for example, cuda-device)\n",
        "            Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b = Energy_b.to(device), \\\n",
        "                                            UVW_b.to(device), \\\n",
        "                                            UVWREC_b.to(device), \\\n",
        "                                            DIR_b.to(device), \\\n",
        "                                            ADD_b.to(device), \\\n",
        "                                            PMResponse_b.to(device)\n",
        "\n",
        "#             pred = regressor(EnergyDeposit_b)\n",
        "            model.train()\n",
        "            UVWDIR_b = torch.cat([UVW_b,DIR_b],dim=1)\n",
        "            UVWDIRrec_b = torch.cat([UVWREC_b,DIR_b],dim=1)\n",
        "            # pred = model(PMResponse_b,UVWREC_b)\n",
        "            # pred = model(PMResponse_b,torch.zeros(UVW_b.shape[0],3).to(device))\n",
        "            pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "            # loss        = loss_fn(pred, torch.cat([ParticleMomentum_b, ParticlePoint_b], dim=1))\n",
        "            # loss        = loss_fn(pred, torch.cat([Energy_b,ZRP_b],dim=1))\n",
        "            loss        = loss_fn(pred, UVW_b)\n",
        "            loss_rec    = loss_fn(UVWREC_b, UVW_b)\n",
        "            # loss        = loss_fn(pred[:,2], UVW_b[:,2])\n",
        "            # loss_rec    = loss_fn(UVWREC_b[:,2], UVW_b[:,2])\n",
        "            # loss        = loss_fn(pred, Energy_b)\n",
        "\n",
        "            # model.train()\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            train_mse       = metric_relative_mse(pred, UVW_b).item()\n",
        "            train_mse_rec   = metric_relative_mse(UVWREC_b, UVW_b).item()\n",
        "            \n",
        "        experiment.log_metric(\"train_loss\", loss.item(),step=epoch)\n",
        "        experiment.log_metric(\"train_mse\", train_mse,step=epoch)\n",
        "        experiment.log_metric(\"train_loss_rec\", loss_rec.item(),step=epoch)\n",
        "        experiment.log_metric(\"train_mse_rec\", train_mse_rec,step=epoch)\n",
        "\n",
        "        if epoch%10==0:\n",
        "            plt.figure(figsize=(20,12))\n",
        "            grid = plt.GridSpec(3, 3, wspace=0.4, hspace=0.3)\n",
        "            for idim in range(3):\n",
        "                plt.subplot(grid[2,idim])\n",
        "                plt.hist2d(UVW_b[:,2].cpu().detach().numpy(),UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),\n",
        "                        range=[[-1,1],[-0.3,0.3]],\n",
        "                        bins = [40,40]\n",
        "                        )\n",
        "\n",
        "        for Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b in val_dataloader:\n",
        "    # moving them to device(for example, cuda-device)\n",
        "            Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b = Energy_b.to(device), \\\n",
        "                                            UVW_b.to(device), \\\n",
        "                                            UVWREC_b.to(device), \\\n",
        "                                            DIR_b.to(device), \\\n",
        "                                            ADD_b.to(device), \\\n",
        "                                            PMResponse_b.to(device)\n",
        "\n",
        "            break\n",
        "\n",
        "#             pred = regressor(EnergyDeposit_b)\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            UVWDIR_b = torch.cat([UVW_b,DIR_b],dim=1)\n",
        "            UVWDIRrec_b = torch.cat([UVWREC_b,DIR_b],dim=1)\n",
        "            # pred = model(PMResponse_b,UVWREC_b)\n",
        "            # pred = model(PMResponse_b,torch.zeros(UVW_b.shape[0],3).to(device))\n",
        "            pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "            # loss        = loss_fn(pred, torch.cat([ParticleMomentum_b, ParticlePoint_b], dim=1))\n",
        "            # val_loss        = loss_fn(pred, torch.cat([Energy_b,ZRP_b],dim=1))\n",
        "            # val_loss        = loss_fn(pred, UVWDIR_b)\n",
        "            val_loss        = loss_fn(pred, UVW_b)\n",
        "            val_loss_rec    = loss_fn(UVWREC_b, UVW_b)\n",
        "            # val_loss        = loss_fn(pred[:,2], UVW_b[:,2])\n",
        "            # val_loss_rec    = loss_fn(UVWREC_b[:,2], UVW_b[:,2])\n",
        "            # val_loss        = loss_fn(pred, Energy_b)\n",
        "#             loss        = loss_fn(pred, ParticleType_b)\n",
        "            # val_mse   = metric_relative_mse(pred, torch.cat([Energy_b, ZRP_b], dim=1)).item()\n",
        "            val_mse       = metric_relative_mse(pred, UVW_b).item()\n",
        "            val_mse_rec   = metric_relative_mse(UVWREC_b, UVW_b).item()\n",
        "            # val_mse   = metric_relative_mse(pred, Energy_b).item()\n",
        "            # val_mse   = 0\n",
        "        experiment.log_metric(\"val_loss\", val_loss.item(),step=epoch)\n",
        "        experiment.log_metric(\"val_loss_rec\", val_loss_rec.item(),step=epoch)\n",
        "        experiment.log_metric(\"val_mse\", val_mse,step=epoch)\n",
        "        experiment.log_metric(\"val_mse_rec\", val_mse_rec,step=epoch)\n",
        "        resolution =[np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy())**2)) for idim in range(3)]\n",
        "        experiment.log_metric(\"u_resolution\", resolution[0],step=epoch)\n",
        "        experiment.log_metric(\"v_resolution\", resolution[1],step=epoch)\n",
        "        experiment.log_metric(\"w_resolution\", resolution[2],step=epoch)\n",
        "        if epoch%10==0:\n",
        "            # plt.figure(figsize=(20,12))\n",
        "            # grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\n",
        "            for idim in range(3):\n",
        "                plt.subplot(grid[0,idim])\n",
        "                plt.hist(UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),range=(-0.5,0.5),bins=80)\n",
        "                plt.title(\"%f\"%(np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy())**2))))\n",
        "                plt.subplot(grid[1,idim])\n",
        "                plt.hist2d(UVW_b[:,2].cpu().detach().numpy(),UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),\n",
        "                        range=[[-1,1],[-0.3,0.3]],\n",
        "                        bins = [40,40]\n",
        "                        )\n",
        "            \n",
        "            experiment.log_figure(figure=plt)\n",
        "            plt.close()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp53YVEDD3Tz",
        "colab_type": "code",
        "outputId": "18d79361-bc4e-4f82-fb0a-1956a8c62bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "with experiment.train():\n",
        "    run_training(10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/10000 [00:07<20:15:50,  7.30s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 11/10000 [00:54<14:48:03,  5.33s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 31/10000 [02:30<14:52:42,  5.37s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 41/10000 [03:18<14:54:01,  5.39s/it]COMET ERROR: File could not be uploaded\n",
            "  1%|          | 51/10000 [04:05<14:50:24,  5.37s/it]COMET ERROR: File could not be uploaded\n",
            "  1%|          | 54/10000 [04:19<13:21:49,  4.84s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVzAybMWFJ7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}