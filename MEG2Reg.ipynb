{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MEG2Reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satoruk-icepp/MEG2XEC/blob/master/MEG2Reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdCOQWBz1SuB",
        "colab_type": "code",
        "outputId": "92883633-6ec6-4dd6-b2f2-1cb4ffe28de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkZmERLgEDe2",
        "colab_type": "code",
        "outputId": "f3bed3c6-53e7-41ab-9049-60a7973b5b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile .comet.config\n",
        "[comet]\n",
        "api_key=mIel5ZAPOioTs0Cij75dSSQXs\n",
        "logging_file = /tmp/comet.log\n",
        "logging_file_level = info\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting .comet.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIeCSaaMLuiR",
        "colab_type": "code",
        "outputId": "5633d2c7-ef62-4334-d5cd-756db09d383d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install skorch comet_ml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.21.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.4)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.56.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.21.0)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: comet-git-pure>=0.19.11 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.19.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTfFz2Z81Tz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from comet_ml import Experiment\n",
        "from comet_ml.exceptions import InterruptedExperiment\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR,StepLR,CyclicLR,CosineAnnealingLR\n",
        "from torch.utils.data.dataset import Subset\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from scipy.optimize import curve_fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYAOaiY1nWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout.csv')\n",
        "# names=['E','U','V','W','PT','PP','UR','VR','WR']\n",
        "# PMnames = ['PM%d'%(i) for i in range(4760)]\n",
        "# names = names+PMnames\n",
        "# csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout_norm.csv',names=names)\n",
        "csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout_norm.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opEPrKJbzG8A",
        "colab_type": "code",
        "outputId": "694aeca8-afc4-47a6-ad7e-4d4123394370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "experiment = Experiment(project_name=\"CWreg\",log_code=True)\n",
        "curtime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "experiment.set_name(\"%s\"%(curtime))\n",
        "params={'batch_size' : 200,\n",
        "        'train_size':20000,\n",
        "        'val_size':1000,\n",
        "        'dropout_conv':0.4,\n",
        "        'dropout_fc':0.05,\n",
        "        'optim':\"Adam\",\n",
        "        'weight_decay':1e-05,\n",
        "        'Nresblock':5,\n",
        "        'Nsd':0,\n",
        "        'Wthreshold':np.log(0.2+1e-02)/2.5+1,\n",
        "        'ethreshold':0.0,\n",
        "        'weightstd':0.01,\n",
        "        'Nlayer':32,\n",
        "        'Nfc':0,\n",
        "        'Nfcnodes':128,\n",
        "        'Nepoch':5000,\n",
        "        # 'learning_rate':0.0000001,# for lr_test\n",
        "        # 'LRgamma':10,# for lr_test\n",
        "        # 'stepsize_lr':1, # for lr_test\n",
        "        # \"LRtype\":\"Step\",\n",
        "        'learning_rate':0.01,\n",
        "        # 'LRgamma':0.3,\n",
        "        'LRgamma':0.995,\n",
        "        'milestones':[200,400,600,800,1000],\n",
        "        # \"LRtype\":\"MStep\",\n",
        "        # \"LRtype\":\"Cyclic\",\n",
        "        # \"LRtype\":\"CosA\",\n",
        "        \"LRtype\":\"CosExp\",\n",
        "        'stepsize_lr':100,\n",
        "        'stepsize_lr_down':29,\n",
        "        \n",
        "        'base_lr':0.000001,\n",
        "        # 'max_lr':0.005,\n",
        "        \n",
        "        'UseLPF':True\n",
        "}\n",
        "experiment.log_parameters(params)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/satoruk-icepp/cwreg/7c8ef54dee1e48e1b69346b820baef05\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUcYFNGQCMT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o880tO_j2LsK",
        "colab_type": "code",
        "outputId": "2b0827ba-122e-4037-c77e-327fab0b4265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "print(csv_data.shape)\n",
        "print(params['Wthreshold'])\n",
        "# csv_data = csv_data[csv_data['w']<params['Wthreshold']]\n",
        "csv_data = csv_data[csv_data['Energy']>params['ethreshold']]\n",
        "csv_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(88682, 4774)\n",
            "0.3757409006941327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Energy</th>\n",
              "      <th>EneRec</th>\n",
              "      <th>u</th>\n",
              "      <th>v</th>\n",
              "      <th>w</th>\n",
              "      <th>ptheta</th>\n",
              "      <th>pphi</th>\n",
              "      <th>smag</th>\n",
              "      <th>stheta</th>\n",
              "      <th>sphi</th>\n",
              "      <th>urec</th>\n",
              "      <th>vrec</th>\n",
              "      <th>wrec</th>\n",
              "      <th>PM_0</th>\n",
              "      <th>PM_1</th>\n",
              "      <th>PM_2</th>\n",
              "      <th>PM_3</th>\n",
              "      <th>PM_4</th>\n",
              "      <th>PM_5</th>\n",
              "      <th>PM_6</th>\n",
              "      <th>PM_7</th>\n",
              "      <th>PM_8</th>\n",
              "      <th>PM_9</th>\n",
              "      <th>PM_10</th>\n",
              "      <th>PM_11</th>\n",
              "      <th>PM_12</th>\n",
              "      <th>PM_13</th>\n",
              "      <th>PM_14</th>\n",
              "      <th>PM_15</th>\n",
              "      <th>PM_16</th>\n",
              "      <th>PM_17</th>\n",
              "      <th>PM_18</th>\n",
              "      <th>PM_19</th>\n",
              "      <th>PM_20</th>\n",
              "      <th>PM_21</th>\n",
              "      <th>PM_22</th>\n",
              "      <th>PM_23</th>\n",
              "      <th>PM_24</th>\n",
              "      <th>PM_25</th>\n",
              "      <th>PM_26</th>\n",
              "      <th>...</th>\n",
              "      <th>PM_4721</th>\n",
              "      <th>PM_4722</th>\n",
              "      <th>PM_4723</th>\n",
              "      <th>PM_4724</th>\n",
              "      <th>PM_4725</th>\n",
              "      <th>PM_4726</th>\n",
              "      <th>PM_4727</th>\n",
              "      <th>PM_4728</th>\n",
              "      <th>PM_4729</th>\n",
              "      <th>PM_4730</th>\n",
              "      <th>PM_4731</th>\n",
              "      <th>PM_4732</th>\n",
              "      <th>PM_4733</th>\n",
              "      <th>PM_4734</th>\n",
              "      <th>PM_4735</th>\n",
              "      <th>PM_4736</th>\n",
              "      <th>PM_4737</th>\n",
              "      <th>PM_4738</th>\n",
              "      <th>PM_4739</th>\n",
              "      <th>PM_4740</th>\n",
              "      <th>PM_4741</th>\n",
              "      <th>PM_4742</th>\n",
              "      <th>PM_4743</th>\n",
              "      <th>PM_4744</th>\n",
              "      <th>PM_4745</th>\n",
              "      <th>PM_4746</th>\n",
              "      <th>PM_4747</th>\n",
              "      <th>PM_4748</th>\n",
              "      <th>PM_4749</th>\n",
              "      <th>PM_4750</th>\n",
              "      <th>PM_4751</th>\n",
              "      <th>PM_4752</th>\n",
              "      <th>PM_4753</th>\n",
              "      <th>PM_4754</th>\n",
              "      <th>PM_4755</th>\n",
              "      <th>PM_4756</th>\n",
              "      <th>PM_4757</th>\n",
              "      <th>PM_4758</th>\n",
              "      <th>PM_4759</th>\n",
              "      <th>flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.906775</td>\n",
              "      <td>0.647425</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.406430</td>\n",
              "      <td>-0.107640</td>\n",
              "      <td>-0.157894</td>\n",
              "      <td>0.026197</td>\n",
              "      <td>0.372978</td>\n",
              "      <td>-0.403479</td>\n",
              "      <td>0.611859</td>\n",
              "      <td>0.407013</td>\n",
              "      <td>0.386388</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.771196</td>\n",
              "      <td>0.753837</td>\n",
              "      <td>-0.550071</td>\n",
              "      <td>0.363598</td>\n",
              "      <td>-0.479323</td>\n",
              "      <td>0.103068</td>\n",
              "      <td>-0.142511</td>\n",
              "      <td>0.149667</td>\n",
              "      <td>0.009527</td>\n",
              "      <td>-0.171185</td>\n",
              "      <td>-0.551514</td>\n",
              "      <td>0.366742</td>\n",
              "      <td>-0.548834</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.908030</td>\n",
              "      <td>0.224333</td>\n",
              "      <td>-0.437471</td>\n",
              "      <td>0.610209</td>\n",
              "      <td>-0.035703</td>\n",
              "      <td>0.171466</td>\n",
              "      <td>0.106931</td>\n",
              "      <td>-0.071142</td>\n",
              "      <td>0.283655</td>\n",
              "      <td>0.228618</td>\n",
              "      <td>-0.453216</td>\n",
              "      <td>0.693381</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.750934</td>\n",
              "      <td>0.762915</td>\n",
              "      <td>0.642363</td>\n",
              "      <td>0.384732</td>\n",
              "      <td>0.575611</td>\n",
              "      <td>-0.100793</td>\n",
              "      <td>-0.150795</td>\n",
              "      <td>0.110528</td>\n",
              "      <td>0.355876</td>\n",
              "      <td>-0.884711</td>\n",
              "      <td>0.591816</td>\n",
              "      <td>0.380710</td>\n",
              "      <td>0.581872</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.930512</td>\n",
              "      <td>-0.244407</td>\n",
              "      <td>0.660570</td>\n",
              "      <td>0.186308</td>\n",
              "      <td>0.044294</td>\n",
              "      <td>-0.258908</td>\n",
              "      <td>0.104042</td>\n",
              "      <td>0.067992</td>\n",
              "      <td>-0.113370</td>\n",
              "      <td>-0.247431</td>\n",
              "      <td>0.654285</td>\n",
              "      <td>0.209709</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88677</th>\n",
              "      <td>0.847906</td>\n",
              "      <td>0.896458</td>\n",
              "      <td>-0.381009</td>\n",
              "      <td>0.677956</td>\n",
              "      <td>0.529823</td>\n",
              "      <td>0.062263</td>\n",
              "      <td>-0.265723</td>\n",
              "      <td>0.024990</td>\n",
              "      <td>0.233780</td>\n",
              "      <td>-0.255067</td>\n",
              "      <td>-0.391716</td>\n",
              "      <td>0.679765</td>\n",
              "      <td>0.501417</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88678</th>\n",
              "      <td>0.633703</td>\n",
              "      <td>0.648628</td>\n",
              "      <td>0.682045</td>\n",
              "      <td>-0.469112</td>\n",
              "      <td>0.429662</td>\n",
              "      <td>-0.112177</td>\n",
              "      <td>0.183867</td>\n",
              "      <td>0.019711</td>\n",
              "      <td>0.182112</td>\n",
              "      <td>0.326817</td>\n",
              "      <td>0.667692</td>\n",
              "      <td>-0.469394</td>\n",
              "      <td>0.389050</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88679</th>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.920071</td>\n",
              "      <td>0.437220</td>\n",
              "      <td>0.603339</td>\n",
              "      <td>0.414835</td>\n",
              "      <td>-0.074034</td>\n",
              "      <td>-0.236477</td>\n",
              "      <td>0.021785</td>\n",
              "      <td>-0.108743</td>\n",
              "      <td>-0.158606</td>\n",
              "      <td>0.435893</td>\n",
              "      <td>0.600493</td>\n",
              "      <td>0.376557</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88680</th>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.873334</td>\n",
              "      <td>-0.438076</td>\n",
              "      <td>-0.419423</td>\n",
              "      <td>-0.247182</td>\n",
              "      <td>0.082201</td>\n",
              "      <td>0.164391</td>\n",
              "      <td>0.044831</td>\n",
              "      <td>0.104264</td>\n",
              "      <td>0.107879</td>\n",
              "      <td>-0.448017</td>\n",
              "      <td>-0.415727</td>\n",
              "      <td>-0.165436</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88681</th>\n",
              "      <td>0.738617</td>\n",
              "      <td>0.706318</td>\n",
              "      <td>-0.304523</td>\n",
              "      <td>0.098590</td>\n",
              "      <td>-0.719897</td>\n",
              "      <td>0.058867</td>\n",
              "      <td>-0.038642</td>\n",
              "      <td>0.077628</td>\n",
              "      <td>-0.159758</td>\n",
              "      <td>0.022532</td>\n",
              "      <td>-0.302675</td>\n",
              "      <td>0.097457</td>\n",
              "      <td>-0.526599</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0083</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0069</td>\n",
              "      <td>0.0069</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88682 rows  4774 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Energy    EneRec         u         v  ...  PM_4757  PM_4758  PM_4759  flag\n",
              "0      0.881000  0.906775  0.647425  0.402845  ...   0.0020   0.0014   0.0012     0\n",
              "1      0.771196  0.753837 -0.550071  0.363598  ...   0.0010   0.0016   0.0011     0\n",
              "2      0.881000  0.908030  0.224333 -0.437471  ...   0.0032   0.0033   0.0056     0\n",
              "3      0.750934  0.762915  0.642363  0.384732  ...   0.0015   0.0019   0.0013     0\n",
              "4      0.881000  0.930512 -0.244407  0.660570  ...   0.0005   0.0004   0.0005     0\n",
              "...         ...       ...       ...       ...  ...      ...      ...      ...   ...\n",
              "88677  0.847906  0.896458 -0.381009  0.677956  ...   0.0006   0.0009   0.0005     0\n",
              "88678  0.633703  0.648628  0.682045 -0.469112  ...   0.0019   0.0013   0.0014     0\n",
              "88679  0.881000  0.920071  0.437220  0.603339  ...   0.0009   0.0008   0.0006     0\n",
              "88680  0.881000  0.873334 -0.438076 -0.419423  ...   0.0024   0.0034   0.0030     0\n",
              "88681  0.738617  0.706318 -0.304523  0.098590  ...   0.0035   0.0037   0.0043     0\n",
              "\n",
              "[88682 rows x 4774 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E8pdRH32YPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_data_numpy = csv_data.to_numpy()\n",
        "del csv_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCD9VPG5UBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy  = csv_data_numpy[:,0]\n",
        "EneREC  = csv_data_numpy[:,1]\n",
        "UVW     = csv_data_numpy[:,2:5]\n",
        "DIR     = csv_data_numpy[:,5:7]\n",
        "SHW     = csv_data_numpy[:,7:10]\n",
        "UVWREC  = csv_data_numpy[:,10:13]\n",
        "PMResponse = csv_data_numpy[:,13:4773]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUpGkRmH5xUl",
        "colab_type": "code",
        "outputId": "808c86c2-b335-4d2a-d0cd-4b9d9a979359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "Energy     = Energy.reshape(-1,1)\n",
        "EneREC     = EneREC.reshape(-1,1)\n",
        "PMResponse_MPPC,PMResponse_PMT = np.split(PMResponse,[4092],axis=1)\n",
        "PMResponse_MPPC = PMResponse_MPPC.reshape(-1,93,44)\n",
        "indx_U = np.arange(-21.5,22.5)\n",
        "indx_V = np.arange(-46,47)\n",
        "# for i in np.arange(-21.5,22.5):\n",
        "PMU = (np.dot(np.sum(PMResponse_MPPC[:],axis=1),indx_U)/np.sum(np.sum(PMResponse_MPPC[:],axis=1),axis=1))/22\n",
        "PMV = (np.dot(np.sum(PMResponse_MPPC[:],axis=2),indx_V)/np.sum(np.sum(PMResponse_MPPC[:],axis=2),axis=1))/46.5\n",
        "PMU = PMU.reshape(-1,1)\n",
        "PMV = PMV.reshape(-1,1)\n",
        "PMURMS = np.sqrt(np.mean(np.square(np.sum(PMResponse_MPPC[:],axis=1)),axis=1)).reshape(-1,1)\n",
        "PMVRMS = np.sqrt(np.mean(np.square(np.sum(PMResponse_MPPC[:],axis=2)),axis=1)).reshape(-1,1)\n",
        "plt.hist2d(UVW[:,2],PMURMS.reshape(-1),bins=[40,40])\n",
        "ADD = np.concatenate((PMU,PMV,PMURMS,PMVRMS),axis=1)\n",
        "COSST = 0.8*np.cos(np.pi*SHW[:,1]).reshape(-1,1)\n",
        "SINST = 0.8*np.sin(np.pi*SHW[:,1]).reshape(-1,1)\n",
        "COSSP = 0.8*np.cos(np.pi*SHW[:,2]).reshape(-1,1)\n",
        "SINSP = 0.8*np.sin(np.pi*SHW[:,2]).reshape(-1,1)\n",
        "SHW = np.concatenate((SHW[:,0].reshape(-1,1),COSST,SINST,COSSP,SINSP),axis=1)\n",
        "# print(ADD)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlZRTgBmZhX1/wGXwbo5UdVBlAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2PjwECK5kL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UVW = np.divide(UVW,[2,1,16])\n",
        "# UVWREC = np.divide(UVWREC,[2,1,16])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr8qby97dTE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del csv_data_numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBCtjK5S_ro4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PMResponse = PMResponse.reshape(-1,4760)\n",
        "# PMResponse = PMResponse/PMResponseScale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU_4kZYGChlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy        = torch.tensor(Energy).float()\n",
        "EneREC        = torch.tensor(EneREC).float()\n",
        "UVW           = torch.tensor(UVW).float()\n",
        "DIR           = torch.tensor(DIR).float()\n",
        "SHW           = torch.tensor(SHW).float()\n",
        "ADD           = torch.tensor(ADD).float()\n",
        "UVWREC        = torch.tensor(UVWREC).float()\n",
        "PMResponse    = torch.tensor(PMResponse).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGwOp69_eMd",
        "colab_type": "code",
        "outputId": "ea2d7f87-66a7-464e-8494-3dbe3acc8267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from torch.utils.data.dataset import Subset\n",
        "BATCH_SIZE = params[\"batch_size\"]\n",
        "calo_dataset    = utils.TensorDataset(Energy,EneREC,UVW,UVWREC,DIR,SHW,ADD,PMResponse)\n",
        "data_size =  len(calo_dataset)\n",
        "full_size = int(data_size/1000)*1000\n",
        "val_size = 500\n",
        "print(data_size)\n",
        "# train_dataset = Subset(calo_dataset,list(range(0,full_size-val_size)))\n",
        "# val_dataset = Subset(calo_dataset,list(range(full_size-val_size,full_size)))\n",
        "train_dataset = Subset(calo_dataset,list(range(0,params[\"train_size\"])))\n",
        "val_dataset = Subset(calo_dataset,list(range(params[\"train_size\"],params[\"train_size\"]+params[\"val_size\"])))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                              batch_size=BATCH_SIZE, \n",
        "                                              pin_memory=True, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                              batch_size=len(val_dataset), \n",
        "                                              pin_memory=True, shuffle=True)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xa9GjOIattu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d, nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TiG4ON2EEi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(ResidualBlock, self).__init__()        \n",
        "        self.conv1 = nn.Conv2d(input_size,input_size,3,padding=1)\n",
        "        self.conv2 = nn.Conv2d(input_size,input_size,3,padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)        \n",
        "        self.activation = nn.LeakyReLU(0.0)\n",
        "    def forward(self,xraw):\n",
        "        x = self.activation(self.bn1(self.conv1(xraw)))\n",
        "        x = self.activation(self.bn2(self.conv2(x))+xraw)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcu5WfPmLmpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DoubleSize(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DoubleSize, self).__init__()\n",
        "        eps = 1e-3\n",
        "        self.ups = nn.Upsample(scale_factor = 2,mode = 'bilinear',align_corners=False )\n",
        "    def forward(self,x):\n",
        "        return self.ups(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0QEoDZM8OE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Regressor(nn.Module):\n",
        "    def __init__(self, dropout_conv =0.0,dropout_fc=0.0,Nresblock=0,Nsd=0,Nlayer=32,Nfc=4,Nfcnodes=256):\n",
        "        super(Regressor, self).__init__()\n",
        "        # self.rconv1 = ReducedConv(1,Nlayer,)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, \n",
        "            Nlayer, \n",
        "            kernel_size=(6,4), \n",
        "            stride = (3,2), \n",
        "            padding = (3,3)\n",
        "            )#(93+6,44+6)->32,24\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            self.conv1.out_channels, \n",
        "            self.conv1.out_channels*2, \n",
        "            kernel_size=(3, 4), \n",
        "            stride=(3,2)\n",
        "            ,padding = (2,1)\n",
        "        )#32*24->12*23\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            self.conv2.out_channels, \n",
        "            self.conv2.out_channels*2, \n",
        "            2\n",
        "            ,stride=2\n",
        "        )#12*12->6*6\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            self.conv3.out_channels, \n",
        "            self.conv3.out_channels*2, \n",
        "            2\n",
        "            ,stride=2\n",
        "        )#12*12->6*6\n",
        "        self.convsd = nn.Conv2d(\n",
        "            self.conv1.out_channels, \n",
        "            self.conv1.out_channels, \n",
        "            3,\n",
        "            padding=1\n",
        "        )#6*6->6*3 \n",
        "        self.rb = ResidualBlock(self.conv1.out_channels)\n",
        "        # self.fcstart = nn.Linear(self.conv3.out_channels*18+668+7,Nfcnodes)\n",
        "        self.fcstart = nn.Linear(self.conv4.out_channels*9+668+7,Nfcnodes)\n",
        "        self.Nfc = Nfc\n",
        "        self.fc=[nn.Linear(self.fcstart.out_features//2**i,self.fcstart.out_features//2**(i+1)).to(device) for i in range(self.Nfc)]\n",
        "        self.fcend = nn.Linear(self.fcstart.out_features//2**(Nfc),9)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(self.conv3.out_channels)\n",
        "        self.bn4 = nn.BatchNorm2d(self.conv4.out_channels)\n",
        "        self.dropout1 = nn.Dropout(dropout_conv)\n",
        "        self.dropoutfc = nn.Dropout(dropout_fc)\n",
        "        self.Nresblock = Nresblock\n",
        "        self.Nsd = Nsd\n",
        "        \n",
        "    def forward(self, x, add):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x_mppc,x_pmt = torch.split(x,4092,dim=1)\n",
        "        x_mppc = x_mppc.view(x_mppc.shape[0],1,93,44)\n",
        "        x_mppc = F.relu(self.bn1(self.conv1(x_mppc)))\n",
        "        for i in range(self.Nresblock):\n",
        "            x_mppc = self.dropout1(self.rb(x_mppc))\n",
        "        for i in range(self.Nsd):\n",
        "            x_mppc = F.relu(self.dropout1(self.bn1(self.convsd(x_mppc))))\n",
        "        x_mppc = F.relu(self.dropout1(self.bn2(self.conv2(x_mppc))))\n",
        "        x_mppc = F.relu(self.dropout1(self.bn3(self.conv3(x_mppc))))\n",
        "        x_mppc = F.relu(self.dropout1(self.bn4(self.conv4(x_mppc))))\n",
        "        \n",
        "        x_mppc = x_mppc.view(x_mppc.shape[0],self.conv4.out_channels*9)\n",
        "        x = torch.cat([x_mppc,x_pmt,add],dim=1)\n",
        "        x = F.relu(self.fcstart(x))\n",
        "        for i in range(self.Nfc):\n",
        "            x = F.relu(self.dropoutfc(self.fc[i](x)))\n",
        "        # x = F.relu(self.dropoutfc(self.fc3(x)))\n",
        "        # x = F.relu(self.dropoutfc(self.fc4(x)))\n",
        "        x = self.fcend(x)\n",
        "        return torch.tanh(x)\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHrkeby_Xde",
        "colab_type": "code",
        "outputId": "0f3afa7e-81be-4172-b405-7a77b9760a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = Regressor(\n",
        "    params[\"dropout_conv\"],\n",
        "    params[\"dropout_fc\"],\n",
        "    params[\"Nresblock\"],\n",
        "    params[\"Nsd\"],\n",
        "    params['Nlayer'],\n",
        "    params['Nfc'],\n",
        "    params['Nfcnodes']\n",
        "    ).to(device)\n",
        "    \n",
        "print(model)\n",
        "model.weight_init(mean=0.0, std=params['weightstd'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressor(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(6, 4), stride=(3, 2), padding=(3, 3))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 4), stride=(3, 2), padding=(2, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (convsd): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (rb): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (activation): LeakyReLU(negative_slope=0.0)\n",
            "  )\n",
            "  (fcstart): Linear(in_features=2979, out_features=128, bias=True)\n",
            "  (fcend): Linear(in_features=128, out_features=9, bias=True)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.4, inplace=False)\n",
            "  (dropoutfc): Dropout(p=0.05, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhlkX3koCV5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Energy_,UVW_,UVWREC_,DIR_,ADD_,PMResponse_ = next(iter(train_dataloader))\n",
        "# writer = SummaryWriter()\n",
        "# writer.add_graph(model, (PMResponse_.to(device), torch.cat([ADD_.to(device),UVWREC_.to(device)],dim=1)))\n",
        "# writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_qTaaIERJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs/tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tww2W1vlmqEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineExpLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, T_max, eta_min=0, gamma=0.99, last_epoch=-1):\n",
        "            self.T_max = T_max\n",
        "            self.eta_min = eta_min\n",
        "            # self.decayconst = decayconst\n",
        "            self.gamma = gamma\n",
        "            super(CosineExpLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [self.gamma ** self.last_epoch*(self.eta_min + (base_lr - self.eta_min) *\n",
        "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2)\n",
        "                for base_lr in self.base_lrs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsT0tiw2Cq0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning_rate = 0.001\n",
        "# opt = optim.Adam(regressor.parameters(), lr=learning_rate)\n",
        "opt = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
        "if params[\"LRtype\"]==\"Cyclic\":\n",
        "    scheduler = CyclicLR(opt, params[\"base_lr\"],params[\"learning_rate\"],\n",
        "                         step_size_up=params[\"stepsize_lr\"],\n",
        "                         step_size_down=params[\"stepsize_lr_down\"],\n",
        "                         cycle_momentum=False,mode=\"exp_range\",gamma = params[\"LRgamma\"])\n",
        "elif params[\"LRtype\"]==\"MStep\":\n",
        "    scheduler = MultiStepLR(opt, milestones=params[\"milestones\"], gamma=params[\"LRgamma\"])\n",
        "elif params[\"LRtype\"]==\"Step\":\n",
        "    scheduler = StepLR(opt,step_size=params[\"stepsize_lr\"],gamma=params[\"LRgamma\"])\n",
        "elif params[\"LRtype\"]==\"CosA\":\n",
        "    scheduler = CosineAnnealingLR(opt,T_max=params[\"stepsize_lr\"],eta_min=params[\"base_lr\"])\n",
        "elif params[\"LRtype\"]==\"CosExp\":\n",
        "    scheduler = CosineExpLR(opt,T_max=params[\"stepsize_lr\"],eta_min=params[\"base_lr\"],gamma = params[\"LRgamma\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV8n2CduHEg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy_mean, UVW_mean, DIR_mean,SHW_mean = Energy.mean(dim=0).to(device), UVW.mean(dim=0).to(device), DIR.mean(dim=0).to(device),SHW.mean(dim=0).to(device)\n",
        "UVWE_mean = torch.cat([UVW_mean,Energy_mean]).to(device)\n",
        "UVWESHW_mean = torch.cat([UVW_mean,Energy_mean,SHW_mean]).to(device)\n",
        "UVWDIR_mean = torch.cat([UVW_mean, DIR_mean]).to(device)\n",
        "def metric_relative_mse(y_pred,y_true):\n",
        "    y_true_mean = y_true.mean(dim=0)\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVWE_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVWESHW_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVWESHW_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVW_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true[:,2] - y_pred[:,2]).pow(2).mean(dim=0) / (y_true[:,2] - UVW_mean[2]).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVWDIR_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - Energy_mean).pow(2).mean(dim=0)).sum()).sqrt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue2O6r-2D1zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_fn = torch.nn.SmoothL1Loss().to(device)\n",
        "loss_fn = torch.nn.L1Loss().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmJdk_POC9Mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranges=[2,2,4,2,3,50,50]\n",
        "\n",
        "def run_training(epochs=100):\n",
        "    # iterating over epochs...\n",
        "    ibatch = 0\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        first = True\n",
        "        with experiment.train():\n",
        "            for Energy_b, EneREC_b, UVW_b, UVWREC_b, DIR_b, SHW_b, ADD_b, PMResponse_b in train_dataloader:\n",
        "            # moving them to device(for example, cuda-device)\n",
        "                Energy_b, EneREC_b, UVW_b, UVWREC_b, DIR_b, SHW_b, ADD_b, PMResponse_b = Energy_b.to(device), \\\n",
        "                                                EneREC_b.to(device), \\\n",
        "                                                UVW_b.to(device), \\\n",
        "                                                UVWREC_b.to(device), \\\n",
        "                                                DIR_b.to(device), \\\n",
        "                                                SHW_b.to(device), \\\n",
        "                                                ADD_b.to(device), \\\n",
        "                                                PMResponse_b.to(device)\n",
        "\n",
        "    #             pred = regressor(EnergyDeposit_b)\n",
        "                model.train()\n",
        "                UVWDIR_b = torch.cat([UVW_b,DIR_b],dim=1)\n",
        "                UVWDIRrec_b = torch.cat([UVWREC_b,DIR_b],dim=1)\n",
        "                if params[\"UseLPF\"]:\n",
        "                    pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "                else:\n",
        "                    pred = model(PMResponse_b,ADD_b)\n",
        "                loss            = loss_fn(pred, torch.cat([UVW_b,Energy_b,SHW_b],dim=1))\n",
        "                loss_rec        = loss_fn(torch.cat([UVWREC_b,EneREC_b,SHW_b],dim=1), torch.cat([UVW_b,Energy_b,SHW_b],dim=1))\n",
        "                # loss        = loss_fn(pred, UVW_b)\n",
        "                # loss_rec    = loss_fn(UVWREC_b, UVW_b)\n",
        "                # loss        = loss_fn(pred, Energy_b)\n",
        "\n",
        "                # model.train()\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                # if params[\"LRtype\"]==\"Cyclic\":\n",
        "                #     scheduler.step()\n",
        "                ibatch+=1\n",
        "            experiment.log_metric(\"learning_rate\", scheduler.get_lr(),step=epoch)\n",
        "            # if params[\"LRtype\"]!=\"Cyclic\":\n",
        "            scheduler.step()\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                if params[\"UseLPF\"]:\n",
        "                    pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "                else:\n",
        "                    pred = model(PMResponse_b,ADD_b)\n",
        "                # train_mse       = metric_relative_mse(pred, UVW_b).item()\n",
        "                # train_mse_rec   = metric_relative_mse(UVWREC_b, UVW_b).item()\n",
        "                train_mse            = metric_relative_mse(pred, torch.cat([UVW_b,Energy_b,SHW_b],dim=1)).item()\n",
        "                train_mse_rec        = metric_relative_mse(torch.cat([UVWREC_b,EneREC_b,SHW_b],dim=1), torch.cat([UVW_b,Energy_b,SHW_b],dim=1)).item()\n",
        "\n",
        "            # if epoch%10==0:\n",
        "            #     plt.figure(figsize=(20,12))\n",
        "            #     grid = plt.GridSpec(3, 3, wspace=0.4, hspace=0.3)\n",
        "            #     for idim in range(3):\n",
        "            #         plt.subplot(grid[2,idim])\n",
        "            #         plt.hist2d(UVW_b[:,2].cpu().detach().numpy(),UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),\n",
        "            #                 range=[[-1,1],[-0.2,0.2]],\n",
        "            #                 bins = [40,40]\n",
        "            #                 )\n",
        "            \n",
        "            \n",
        "            experiment.log_metric(\"train_loss\", loss.item(),step=epoch)\n",
        "            experiment.log_metric(\"train_mse\", train_mse,step=epoch)\n",
        "            experiment.log_metric(\"train_loss_rec\", loss_rec.item(),step=epoch)\n",
        "            experiment.log_metric(\"train_mse_rec\", train_mse_rec,step=epoch)\n",
        "        # scheduler.step()\n",
        "        with experiment.test():\n",
        "            for Energy_b, EneREC_b,  UVW_b, UVWREC_b, DIR_b, SHW_b, ADD_b, PMResponse_b in val_dataloader:\n",
        "        # moving them to device(for example, cuda-device)\n",
        "                Energy_b, EneREC_b,  UVW_b, UVWREC_b, DIR_b, SHW_b, ADD_b, PMResponse_b = Energy_b.to(device), \\\n",
        "                                                EneREC_b.to(device), \\\n",
        "                                                UVW_b.to(device), \\\n",
        "                                                UVWREC_b.to(device), \\\n",
        "                                                DIR_b.to(device), \\\n",
        "                                                SHW_b.to(device), \\\n",
        "                                                ADD_b.to(device), \\\n",
        "                                                PMResponse_b.to(device)\n",
        "\n",
        "                break\n",
        "\n",
        "    #             pred = regressor(EnergyDeposit_b)\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                # UVWDIR_b = torch.cat([UVW_b,DIR_b],dim=1)\n",
        "                # UVWDIRrec_b = torch.cat([UVWREC_b,DIR_b],dim=1)\n",
        "                # pred = model(PMResponse_b,UVWREC_b)\n",
        "                # pred = model(PMResponse_b,torch.zeros(UVW_b.shape[0],3).to(device))\n",
        "                # pred = model(PMResponse_b,ADD_b)\n",
        "                if params[\"UseLPF\"]:\n",
        "                    pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "                else:\n",
        "                    pred = model(PMResponse_b,ADD_b)\n",
        "                # pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "                # val_loss        = loss_fn(pred, UVW_b)\n",
        "                # val_loss_rec    = loss_fn(UVWREC_b, UVW_b)\n",
        "                # val_mse       = metric_relative_mse(pred, UVW_b).item()\n",
        "                # val_mse_rec   = metric_relative_mse(UVWREC_b, UVW_b).item()\n",
        "                # val_loss            = loss_fn(pred, torch.cat([UVW_b,Energy_b],dim=1))\n",
        "                # val_loss_rec        = loss_fn(torch.cat([UVWREC_b,EneREC_b],dim=1), torch.cat([UVW_b,Energy_b],dim=1))\n",
        "                val_loss            = loss_fn(pred, torch.cat([UVW_b,Energy_b,SHW_b],dim=1))\n",
        "                val_loss_rec        = loss_fn(torch.cat([UVWREC_b,EneREC_b,SHW_b],dim=1), torch.cat([UVW_b,Energy_b,SHW_b],dim=1))\n",
        "                val_mse            = metric_relative_mse(pred, torch.cat([UVW_b,Energy_b,SHW_b],dim=1)).item()\n",
        "                val_mse_rec        = metric_relative_mse(torch.cat([UVWREC_b,EneREC_b,SHW_b],dim=1), torch.cat([UVW_b,Energy_b,SHW_b],dim=1)).item()\n",
        "                \n",
        "                # val_mse   = metric_relative_mse(pred, Energy_b).item()\n",
        "                # val_mse   = 0\n",
        "            experiment.log_metric(\"val_loss\", val_loss.item(),step=epoch)\n",
        "            experiment.log_metric(\"val_loss_rec\", val_loss_rec.item(),step=epoch)\n",
        "            experiment.log_metric(\"val_mse\", val_mse,step=epoch)\n",
        "            experiment.log_metric(\"val_mse_rec\", val_mse_rec,step=epoch)\n",
        "            resolution_u = 40*np.sqrt(np.mean((UVW_b[:,0].cpu().detach().numpy()-pred[:,0].cpu().detach().numpy())**2))\n",
        "            resolution_v = 80*np.sqrt(np.mean((UVW_b[:,1].cpu().detach().numpy()-pred[:,1].cpu().detach().numpy())**2))\n",
        "            resolution_w = np.sqrt(np.mean((\n",
        "                40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01)-\n",
        "                40*(np.exp((pred[:,2].cpu().detach().numpy()-1)*2.5)-0.01)\n",
        "                )**2))\n",
        "            \n",
        "            resolution_u_lpf = 40*np.sqrt(np.mean((UVW_b[:,0].cpu().detach().numpy()-UVWREC_b[:,0].cpu().detach().numpy())**2))\n",
        "            resolution_v_lpf = 80*np.sqrt(np.mean((UVW_b[:,1].cpu().detach().numpy()-UVWREC_b[:,1].cpu().detach().numpy())**2))\n",
        "            resolution_w_lpf = np.sqrt(np.mean((\n",
        "                40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01)-\n",
        "                40*(np.exp((UVWREC_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01)\n",
        "                )**2))\n",
        "            resolution_e     = 20*np.sqrt(np.mean((Energy_b[:,0].cpu().detach().numpy()-pred[:,3].cpu().detach().numpy())**2))\n",
        "            resolution_e_raw = 20*np.sqrt(np.mean((Energy_b[:,0].cpu().detach().numpy()-EneREC_b[:,0].cpu().detach().numpy())**2))\n",
        "            dshwl = SHW_b[:,0].cpu().detach().numpy()-pred[:,4].cpu().detach().numpy()\n",
        "            dshwt = np.arctan2(SHW_b[:,1].cpu().detach().numpy(),SHW_b[:,2].cpu().detach().numpy())-np.arctan2(pred[:,5].cpu().detach().numpy(),pred[:,6].cpu().detach().numpy())\n",
        "            dshwp = np.arctan2(SHW_b[:,3].cpu().detach().numpy(),SHW_b[:,4].cpu().detach().numpy())-np.arctan2(pred[:,7].cpu().detach().numpy(),pred[:,8].cpu().detach().numpy())\n",
        "            resolution_shwl = 40*np.sqrt(np.mean((dshwl)**2))\n",
        "            resolution_shwt = 180/math.pi*np.sqrt(np.mean((dshwt[SHW_b[:,0].cpu().detach().numpy()>0.01])**2,axis=0))\n",
        "            resolution_shwp = 180/math.pi*np.sqrt(np.mean((dshwp[SHW_b[:,0].cpu().detach().numpy()>0.01])**2,axis=0))\n",
        "            # fraction = 0\n",
        "            # for i in range(len(Energy_b)):\n",
        "            #     if 180/math.pi*abs(np.arctan2(SHW_b[i,3].cpu().detach().numpy(),SHW_b[i,4].cpu().detach().numpy())-\n",
        "            #                     np.arctan2(pred[i,7].cpu().detach().numpy(),pred[i,8].cpu().detach().numpy()))>50:\n",
        "            #         print(180/math.pi*abs(np.arctan2(SHW_b[i,3].cpu().detach().numpy(),SHW_b[i,4].cpu().detach().numpy())-\n",
        "            #                     np.arctan2(pred[i,7].cpu().detach().numpy(),pred[i,8].cpu().detach().numpy())))\n",
        "            #         fraction+=1/float(len(Energy_b))\n",
        "            # print(fraction)\n",
        "            experiment.log_metric(\"e_resolution\"    , resolution_e    ,step=epoch)\n",
        "            experiment.log_metric(\"e_resolution_raw\", resolution_e_raw,step=epoch)\n",
        "            # resolution     =[np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy())**2)) for idim in range(3)]\n",
        "            # resolution_lpf =[np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-UVWREC_b[:,idim].cpu().detach().numpy())**2)) for idim in range(3)]\n",
        "            experiment.log_metric(\"u_resolution\", resolution_u,step=epoch)\n",
        "            experiment.log_metric(\"v_resolution\", resolution_v,step=epoch)\n",
        "            experiment.log_metric(\"w_resolution\", resolution_w,step=epoch)\n",
        "            experiment.log_metric(\"u_resolution_lpf\", resolution_u_lpf,step=epoch)\n",
        "            experiment.log_metric(\"v_resolution_lpf\", resolution_v_lpf,step=epoch)\n",
        "            experiment.log_metric(\"w_resolution_lpf\", resolution_w_lpf,step=epoch)\n",
        "            experiment.log_metric(\"shwl_resolution\"    , resolution_shwl    ,step=epoch)\n",
        "            experiment.log_metric(\"shwt_resolution\"    , resolution_shwt    ,step=epoch)\n",
        "            experiment.log_metric(\"shwp_resolution\"    , resolution_shwp    ,step=epoch)\n",
        "        if epoch%10==0:\n",
        "            plt.figure(figsize=(28,12))\n",
        "            grid = plt.GridSpec(2, 7, wspace=0.4, hspace=0.3)\n",
        "            for idim in range(7):\n",
        "                if idim==0:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(UVW_b[:,idim].cpu().detach().numpy()*40-\n",
        "                             pred[:,idim].cpu().detach().numpy()*40,\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.hist(UVW_b[:,idim].cpu().detach().numpy()*40-\n",
        "                             UVWREC_b[:,idim].cpu().detach().numpy()*40,\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='r',fill=False,edgecolor='r',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_u))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                               40*(UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy()),\n",
        "                        range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                        bins = [40,40],\n",
        "                            norm=LogNorm()\n",
        "                        )\n",
        "                elif idim==1:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(UVW_b[:,idim].cpu().detach().numpy()*80-\n",
        "                             pred[:,idim].cpu().detach().numpy()*80,\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.hist(UVW_b[:,idim].cpu().detach().numpy()*80-\n",
        "                             UVWREC_b[:,idim].cpu().detach().numpy()*80,\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='r',fill=False,edgecolor='r',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_v))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                               80*(UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy()),\n",
        "                        range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                        bins = [40,40],\n",
        "                            norm=LogNorm()\n",
        "                        )\n",
        "                elif idim==2:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(40*(np.exp((UVW_b[:,idim].cpu().detach().numpy()-1)*2.5)-0.01)-\n",
        "                             40*(np.exp((pred[:,idim].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.hist(40*(np.exp((UVW_b[:,idim].cpu().detach().numpy()-1)*2.5)-0.01)-\n",
        "                             40*(np.exp((UVWREC_b[:,idim].cpu().detach().numpy()-1)*2.5)-0.01)\n",
        "                             ,histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='r',fill=False,edgecolor='r',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_w))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,idim].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                                40*(np.exp((UVW_b[:,idim].cpu().detach().numpy()-1)*2.5)-0.01)-\n",
        "                             40*(np.exp((pred[:,idim].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                            range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                            bins = [40,40],\n",
        "                            norm=LogNorm()\n",
        "                            )\n",
        "                elif idim==3:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(20*(Energy_b[:,0].cpu().detach().numpy()-\n",
        "                             pred[:,idim].cpu().detach().numpy()),\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.hist(20*(Energy_b[:,0].cpu().detach().numpy()-\n",
        "                             EneREC_b[:,0].cpu().detach().numpy()),\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='r',fill=False,edgecolor='r',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_e))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                               20*(Energy_b[:,0].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy()),\n",
        "                        range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                        bins = [40,40],\n",
        "                            norm=LogNorm()\n",
        "                        )\n",
        "                elif idim==4:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(40*(SHW_b[:,0].cpu().detach().numpy()-\n",
        "                             pred[:,idim].cpu().detach().numpy()),\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_shwl))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                               40*(SHW_b[:,0].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy()),\n",
        "                        range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                        bins = [40,40],\n",
        "                            norm=LogNorm()\n",
        "                        )\n",
        "                elif idim==5:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(\n",
        "                        180/math.pi*(\n",
        "                        np.arctan2(SHW_b[:,1].cpu().detach().numpy(),SHW_b[:,2].cpu().detach().numpy())-\n",
        "                        np.arctan2(pred[:,5].cpu().detach().numpy(),pred[:,6].cpu().detach().numpy())),\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_shwt))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                               180/math.pi*(np.arctan2(SHW_b[:,1].cpu().detach().numpy(),SHW_b[:,2].cpu().detach().numpy())-\n",
        "                               np.arctan2(pred[:,5].cpu().detach().numpy(),pred[:,6].cpu().detach().numpy()))\n",
        "                        ,\n",
        "                        range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                        bins = [40,40],\n",
        "                            norm=LogNorm()\n",
        "                        )\n",
        "                elif idim==6:\n",
        "                    plt.subplot(grid[0,idim])\n",
        "                    plt.hist(180/math.pi*(\n",
        "                        np.arctan2(SHW_b[:,3].cpu().detach().numpy(),SHW_b[:,4].cpu().detach().numpy())-\n",
        "                        np.arctan2(pred[:,7].cpu().detach().numpy(),pred[:,8].cpu().detach().numpy())\n",
        "                             ),\n",
        "                             histtype='step',range=(-ranges[idim],ranges[idim]),bins=80,color='b',fill=False,edgecolor='b',lw=2)\n",
        "                    plt.title(\"%f\"%(resolution_shwp))\n",
        "                    plt.subplot(grid[1,idim])\n",
        "                    plt.hist2d(40*(np.exp((UVW_b[:,2].cpu().detach().numpy()-1)*2.5)-0.01),\n",
        "                               180/math.pi*(\n",
        "                                   np.arctan2(SHW_b[:,3].cpu().detach().numpy(),SHW_b[:,4].cpu().detach().numpy())-\n",
        "                                   np.arctan2(pred[:,7].cpu().detach().numpy(),pred[:,8].cpu().detach().numpy())\n",
        "                               ),\n",
        "                                range=[[0,10],[-ranges[idim],ranges[idim]]],\n",
        "                                bins = [40,40],\n",
        "                                norm=LogNorm()\n",
        "                        )\n",
        "                    # plt.hist2d(Energy_b[:,0].cpu().detach().numpy(),\n",
        "                    #            pred[:,idim].cpu().detach().numpy(),\n",
        "                    #     range=[[-1,1],[-1,1]],\n",
        "                    #     bins = [40,40],\n",
        "                    #         norm=LogNorm()\n",
        "                    #     )\n",
        "            #  torch.pred\n",
        "                \n",
        "            \n",
        "            experiment.log_figure(figure=plt)\n",
        "            plt.close()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp53YVEDD3Tz",
        "colab_type": "code",
        "outputId": "e4d9d74d-6854-46dc-8772-f58192bbf13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#  with experiment.train():\n",
        "run_training(params[\"Nepoch\"])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  3%|         | 168/5000 [13:09<6:05:28,  4.54s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv8G78FLfZd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MfM8KEAofR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}