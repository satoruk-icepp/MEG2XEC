{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MEG2Reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satoruk-icepp/MEG2XEC/blob/master/MEG2Reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdCOQWBz1SuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a647cb70-6fa7-44a9-d10c-465f6b5563e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkZmERLgEDe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82510307-604c-431d-b226-3bdd2b44c148"
      },
      "source": [
        "%%writefile .comet.config\n",
        "[comet]\n",
        "api_key=mIel5ZAPOioTs0Cij75dSSQXs\n",
        "logging_file = /tmp/comet.log\n",
        "logging_file_level = info\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting .comet.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wNHxlzNEGOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "60f54946-deeb-486b-972c-a208ef7ae5c8"
      },
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install skorch comet_ml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (2.0.17)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.21.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.3.1)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.2)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.56.0)\n",
            "Requirement already satisfied: comet-git-pure>=0.19.11 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.19.13)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.0)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (2019.9.11)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTfFz2Z81Tz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from comet_ml import Experiment\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from scipy.optimize import curve_fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opEPrKJbzG8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6cb98683-b6c8-4176-96a9-0ad83eae8368"
      },
      "source": [
        "experiment = Experiment(project_name=\"CWreg\")\n",
        "curtime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "experiment.set_name(\"%s\"%(curtime))\n",
        "params={'batch_size' : 200,\n",
        "        'learning_rate':0.1,\n",
        "        'dropout_conv':0.1,\n",
        "        'dropout_fc':0.1,\n",
        "        'optim':\"Adam\",\n",
        "        'weight_decay':1e-05,\n",
        "        'Nresblock':0,\n",
        "        'Nsd':0,\n",
        "        'Wthreshold':np.log(0.2+1e-02)/2.5+1,\n",
        "        'weightstd':0.0001,\n",
        "        'Nlayer':32,\n",
        "        'Nfc':2,\n",
        "        'Nfcnodes':256,\n",
        "        'LRgamma':0.1\n",
        "}\n",
        "experiment.log_parameters(params)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/satoruk-icepp/cwreg/4f1c2c741b8340a59037830014f93f70\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUcYFNGQCMT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYAOaiY1nWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout.csv')\n",
        "# names=['E','U','V','W','PT','PP','UR','VR','WR']\n",
        "# PMnames = ['PM%d'%(i) for i in range(4760)]\n",
        "# names = names+PMnames\n",
        "# csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout_norm.csv',names=names)\n",
        "csv_data  = pd.read_csv('/content/drive/My Drive/MEG2CW/fout_norm.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o880tO_j2LsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "039ba65d-1d13-4cbb-c717-8e0f7cd44507"
      },
      "source": [
        "print(csv_data.shape)\n",
        "print(params['Wthreshold'])\n",
        "# csv_data = csv_data[csv_data['w']<params['Wthreshold']]\n",
        "csv_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(53208, 4770)\n",
            "0.3757409006941327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Energy</th>\n",
              "      <th>u</th>\n",
              "      <th>v</th>\n",
              "      <th>w</th>\n",
              "      <th>ptheta</th>\n",
              "      <th>pphi</th>\n",
              "      <th>urec</th>\n",
              "      <th>vrec</th>\n",
              "      <th>wrec</th>\n",
              "      <th>PM_0</th>\n",
              "      <th>PM_1</th>\n",
              "      <th>PM_2</th>\n",
              "      <th>PM_3</th>\n",
              "      <th>PM_4</th>\n",
              "      <th>PM_5</th>\n",
              "      <th>PM_6</th>\n",
              "      <th>PM_7</th>\n",
              "      <th>PM_8</th>\n",
              "      <th>PM_9</th>\n",
              "      <th>PM_10</th>\n",
              "      <th>PM_11</th>\n",
              "      <th>PM_12</th>\n",
              "      <th>PM_13</th>\n",
              "      <th>PM_14</th>\n",
              "      <th>PM_15</th>\n",
              "      <th>PM_16</th>\n",
              "      <th>PM_17</th>\n",
              "      <th>PM_18</th>\n",
              "      <th>PM_19</th>\n",
              "      <th>PM_20</th>\n",
              "      <th>PM_21</th>\n",
              "      <th>PM_22</th>\n",
              "      <th>PM_23</th>\n",
              "      <th>PM_24</th>\n",
              "      <th>PM_25</th>\n",
              "      <th>PM_26</th>\n",
              "      <th>PM_27</th>\n",
              "      <th>PM_28</th>\n",
              "      <th>PM_29</th>\n",
              "      <th>PM_30</th>\n",
              "      <th>...</th>\n",
              "      <th>PM_4721</th>\n",
              "      <th>PM_4722</th>\n",
              "      <th>PM_4723</th>\n",
              "      <th>PM_4724</th>\n",
              "      <th>PM_4725</th>\n",
              "      <th>PM_4726</th>\n",
              "      <th>PM_4727</th>\n",
              "      <th>PM_4728</th>\n",
              "      <th>PM_4729</th>\n",
              "      <th>PM_4730</th>\n",
              "      <th>PM_4731</th>\n",
              "      <th>PM_4732</th>\n",
              "      <th>PM_4733</th>\n",
              "      <th>PM_4734</th>\n",
              "      <th>PM_4735</th>\n",
              "      <th>PM_4736</th>\n",
              "      <th>PM_4737</th>\n",
              "      <th>PM_4738</th>\n",
              "      <th>PM_4739</th>\n",
              "      <th>PM_4740</th>\n",
              "      <th>PM_4741</th>\n",
              "      <th>PM_4742</th>\n",
              "      <th>PM_4743</th>\n",
              "      <th>PM_4744</th>\n",
              "      <th>PM_4745</th>\n",
              "      <th>PM_4746</th>\n",
              "      <th>PM_4747</th>\n",
              "      <th>PM_4748</th>\n",
              "      <th>PM_4749</th>\n",
              "      <th>PM_4750</th>\n",
              "      <th>PM_4751</th>\n",
              "      <th>PM_4752</th>\n",
              "      <th>PM_4753</th>\n",
              "      <th>PM_4754</th>\n",
              "      <th>PM_4755</th>\n",
              "      <th>PM_4756</th>\n",
              "      <th>PM_4757</th>\n",
              "      <th>PM_4758</th>\n",
              "      <th>PM_4759</th>\n",
              "      <th>flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.647425</td>\n",
              "      <td>0.402845</td>\n",
              "      <td>0.406430</td>\n",
              "      <td>-0.430562</td>\n",
              "      <td>-0.315788</td>\n",
              "      <td>0.611859</td>\n",
              "      <td>0.407013</td>\n",
              "      <td>0.386388</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.154239</td>\n",
              "      <td>-0.550071</td>\n",
              "      <td>0.363598</td>\n",
              "      <td>-0.479323</td>\n",
              "      <td>0.412274</td>\n",
              "      <td>-0.285022</td>\n",
              "      <td>-0.551514</td>\n",
              "      <td>0.366742</td>\n",
              "      <td>-0.548834</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.224333</td>\n",
              "      <td>-0.437471</td>\n",
              "      <td>0.610209</td>\n",
              "      <td>-0.142812</td>\n",
              "      <td>0.342931</td>\n",
              "      <td>0.228618</td>\n",
              "      <td>-0.453216</td>\n",
              "      <td>0.693381</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.150187</td>\n",
              "      <td>0.642363</td>\n",
              "      <td>0.384732</td>\n",
              "      <td>0.575611</td>\n",
              "      <td>-0.403170</td>\n",
              "      <td>-0.301589</td>\n",
              "      <td>0.591816</td>\n",
              "      <td>0.380710</td>\n",
              "      <td>0.581872</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.244407</td>\n",
              "      <td>0.660570</td>\n",
              "      <td>0.186308</td>\n",
              "      <td>0.177176</td>\n",
              "      <td>-0.517817</td>\n",
              "      <td>-0.247431</td>\n",
              "      <td>0.654285</td>\n",
              "      <td>0.209709</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53203</th>\n",
              "      <td>0.165990</td>\n",
              "      <td>0.303824</td>\n",
              "      <td>0.508643</td>\n",
              "      <td>0.122589</td>\n",
              "      <td>-0.221882</td>\n",
              "      <td>-0.398722</td>\n",
              "      <td>0.302143</td>\n",
              "      <td>0.510321</td>\n",
              "      <td>0.138353</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53204</th>\n",
              "      <td>0.150154</td>\n",
              "      <td>-0.148314</td>\n",
              "      <td>0.607137</td>\n",
              "      <td>0.140384</td>\n",
              "      <td>0.108837</td>\n",
              "      <td>-0.475931</td>\n",
              "      <td>-0.164124</td>\n",
              "      <td>0.604553</td>\n",
              "      <td>0.155343</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53205</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.192877</td>\n",
              "      <td>0.433645</td>\n",
              "      <td>-0.114305</td>\n",
              "      <td>0.145891</td>\n",
              "      <td>-0.339932</td>\n",
              "      <td>-0.186460</td>\n",
              "      <td>0.427623</td>\n",
              "      <td>-0.133480</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53206</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.010922</td>\n",
              "      <td>0.507980</td>\n",
              "      <td>0.698484</td>\n",
              "      <td>-0.006670</td>\n",
              "      <td>-0.398203</td>\n",
              "      <td>0.004436</td>\n",
              "      <td>0.524767</td>\n",
              "      <td>0.771661</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53207</th>\n",
              "      <td>0.176200</td>\n",
              "      <td>-0.593930</td>\n",
              "      <td>-0.159663</td>\n",
              "      <td>0.247205</td>\n",
              "      <td>0.412961</td>\n",
              "      <td>0.125159</td>\n",
              "      <td>-0.612349</td>\n",
              "      <td>-0.162239</td>\n",
              "      <td>0.440602</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>53208 rows  4770 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Energy         u         v         w  ...  PM_4757  PM_4758  PM_4759  flag\n",
              "0      0.176200  0.647425  0.402845  0.406430  ...   0.0020   0.0014   0.0012     0\n",
              "1      0.154239 -0.550071  0.363598 -0.479323  ...   0.0010   0.0016   0.0011     0\n",
              "2      0.176200  0.224333 -0.437471  0.610209  ...   0.0032   0.0033   0.0056     0\n",
              "3      0.150187  0.642363  0.384732  0.575611  ...   0.0015   0.0019   0.0013     0\n",
              "4      0.176200 -0.244407  0.660570  0.186308  ...   0.0005   0.0004   0.0005     0\n",
              "...         ...       ...       ...       ...  ...      ...      ...      ...   ...\n",
              "53203  0.165990  0.303824  0.508643  0.122589  ...   0.0011   0.0017   0.0013     0\n",
              "53204  0.150154 -0.148314  0.607137  0.140384  ...   0.0010   0.0004   0.0006     0\n",
              "53205  0.176200 -0.192877  0.433645 -0.114305  ...   0.0013   0.0023   0.0023     0\n",
              "53206  0.176200  0.010922  0.507980  0.698484  ...   0.0012   0.0007   0.0008     0\n",
              "53207  0.176200 -0.593930 -0.159663  0.247205  ...   0.0036   0.0118   0.0046     0\n",
              "\n",
              "[53208 rows x 4770 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E8pdRH32YPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_data_numpy = csv_data.to_numpy()\n",
        "del csv_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCD9VPG5UBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy  = csv_data_numpy[:,0]\n",
        "UVW     = csv_data_numpy[:,1:4]\n",
        "DIR     = csv_data_numpy[:,4:6]\n",
        "UVWREC  = csv_data_numpy[:,6:9]\n",
        "PMResponse = csv_data_numpy[:,9:4101]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUpGkRmH5xUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "740efb40-b103-442a-eb72-a72d43b61be7"
      },
      "source": [
        "Energy     = Energy.reshape(-1,1)\n",
        "PMResponse = PMResponse.reshape(-1,93,44)\n",
        "indx_U = np.arange(-21.5,22.5)\n",
        "indx_V = np.arange(-46,47)\n",
        "# for i in np.arange(-21.5,22.5):\n",
        "PMU = (np.dot(np.sum(PMResponse[:],axis=1),indx_U)/np.sum(np.sum(PMResponse[:],axis=1),axis=1))/22\n",
        "PMV = (np.dot(np.sum(PMResponse[:],axis=2),indx_V)/np.sum(np.sum(PMResponse[:],axis=2),axis=1))/46.5\n",
        "PMU = PMU.reshape(-1,1)\n",
        "PMV = PMV.reshape(-1,1)\n",
        "PMURMS = np.sqrt(np.mean(np.square(np.sum(PMResponse[:],axis=1)),axis=1)).reshape(-1,1)\n",
        "PMVRMS = np.sqrt(np.mean(np.square(np.sum(PMResponse[:],axis=2)),axis=1)).reshape(-1,1)\n",
        "plt.hist2d(UVW[:,2],PMURMS.reshape(-1),bins=[40,40])\n",
        "ADD = np.concatenate((PMU,PMV,PMURMS,PMVRMS),axis=1)\n",
        "# print(ADD)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGZmJfX/AR3K1ikNq5VtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr8qby97dTE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del csv_data_numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBCtjK5S_ro4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PMResponse = PMResponse.reshape(-1,4092)\n",
        "# PMResponse = PMResponse/PMResponseScale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU_4kZYGChlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy        = torch.tensor(Energy).float()\n",
        "UVW           = torch.tensor(UVW).float()\n",
        "DIR           = torch.tensor(DIR).float()\n",
        "ADD           = torch.tensor(ADD).float()\n",
        "UVWREC        = torch.tensor(UVWREC).float()\n",
        "PMResponse    = torch.tensor(PMResponse).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGwOp69_eMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ac432e9-1656-4830-b4e3-31848bff61b6"
      },
      "source": [
        "from torch.utils.data.dataset import Subset\n",
        "BATCH_SIZE = params[\"batch_size\"]\n",
        "calo_dataset    = utils.TensorDataset(Energy,UVW,UVWREC,DIR,ADD,PMResponse)\n",
        "data_size =  len(calo_dataset)\n",
        "full_size = int(data_size/1000)*1000\n",
        "print(data_size)\n",
        "train_dataset = Subset(calo_dataset,list(range(0,int(0.8*full_size))))\n",
        "val_dataset = Subset(calo_dataset,list(range(int(0.8*full_size),full_size)))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                              batch_size=BATCH_SIZE, \n",
        "                                              pin_memory=True, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                              batch_size=len(val_dataset), \n",
        "                                              pin_memory=True, shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xa9GjOIattu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d, nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        if m.bias.data is not None:\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TiG4ON2EEi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(ResidualBlock, self).__init__()        \n",
        "        self.conv1 = nn.Conv2d(input_size,input_size,3,padding=1)\n",
        "        self.conv2 = nn.Conv2d(input_size,input_size,3,padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)        \n",
        "        self.activation = nn.LeakyReLU(0.0)\n",
        "    def forward(self,xraw):\n",
        "        x = self.activation(self.bn1(self.conv1(xraw)))\n",
        "        x = self.activation(self.bn2(self.conv2(x))+xraw)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0QEoDZM8OE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Regressor(nn.Module):\n",
        "    def __init__(self, dropout_conv =0.0,dropout_fc=0.0,Nresblock=0,Nsd=0,Nlayer=32,Nfc=4,Nfcnodes=256):\n",
        "        super(Regressor, self).__init__()\n",
        "        # self.fc1 = nn.Linear(4092,256)\n",
        "        self.conv1 = nn.Conv2d(1, Nlayer, kernel_size=(10,10), stride = 5, padding = (1,3))#(93+6,44+6)->24,12\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            self.conv1.out_channels, \n",
        "            self.conv1.out_channels*2, \n",
        "            kernel_size=(4, 3), \n",
        "            stride=2\n",
        "            # ,padding = (1,1)\n",
        "        )#24*12->12*6\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            self.conv2.out_channels, \n",
        "            self.conv2.out_channels*2, \n",
        "            3\n",
        "            # ,stride=2\n",
        "        )#12*6->6*3\n",
        "        self.convsd = nn.Conv2d(\n",
        "            self.conv1.out_channels, \n",
        "            self.conv1.out_channels, \n",
        "            3,\n",
        "            padding=1\n",
        "        )#12*6->6*3 \n",
        "        self.rb = ResidualBlock(self.conv1.out_channels)\n",
        "        self.fcstart = nn.Linear(self.conv3.out_channels*12+7,Nfcnodes)\n",
        "        self.Nfc = Nfc\n",
        "        self.fc=[nn.Linear(self.fcstart.out_features//2**i,self.fcstart.out_features//2**(i+1)).to(device) for i in range(self.Nfc)]\n",
        "        self.fcend = nn.Linear(self.fcstart.out_features//2**(Nfc),3)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(self.conv3.out_channels)\n",
        "        self.dropout1 = nn.Dropout(dropout_conv)\n",
        "        self.dropoutfc = nn.Dropout(dropout_fc)\n",
        "        self.Nresblock = Nresblock\n",
        "        self.Nsd = Nsd\n",
        "        \n",
        "    def forward(self, x, uvw_rec):\n",
        "        # x = F.relu(self.dropout_fc1(self.fc1(x)))\n",
        "        x = x.view(x.shape[0],1,-1)\n",
        "        \n",
        "        # x_mppc,x_pmt = x.split(4092)\n",
        "        x = x.view(x.shape[0],1,93,44)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        for i in range(self.Nresblock):\n",
        "            x = self.dropout1(self.rb(x))\n",
        "        for i in range(self.Nsd):\n",
        "            x = F.relu(self.dropout1(self.bn1(self.convsd(x))))\n",
        "        x = F.relu(self.dropout1(self.bn2(self.conv2(x))))\n",
        "        x = F.relu(self.dropout1(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        x = x.view(x.shape[0],self.conv3.out_channels*12)\n",
        "        x = torch.cat([x,uvw_rec],dim=1)\n",
        "        x = F.relu(self.fcstart(x))\n",
        "        for i in range(self.Nfc):\n",
        "            x = F.relu(self.dropoutfc(self.fc[i](x)))\n",
        "        # x = F.relu(self.dropoutfc(self.fc3(x)))\n",
        "        # x = F.relu(self.dropoutfc(self.fc4(x)))\n",
        "        x = self.fcend(x)\n",
        "        return torch.tanh(x)\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHrkeby_Xde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "31e1413d-b18e-44cd-ae52-945aaaee157e"
      },
      "source": [
        "model = Regressor(\n",
        "    params[\"dropout_conv\"],\n",
        "    params[\"dropout_fc\"],\n",
        "    params[\"Nresblock\"],\n",
        "    params[\"Nsd\"],\n",
        "    params['Nlayer'],\n",
        "    params['Nfc'],\n",
        "    params['Nfcnodes']\n",
        "    ).to(device)\n",
        "print(model)\n",
        "# model.weight_init(mean=0.0, std=params['weightstd'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressor(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(10, 10), stride=(5, 5), padding=(1, 3))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(4, 3), stride=(2, 2))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (convsd): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (rb): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (activation): LeakyReLU(negative_slope=0.0)\n",
            "  )\n",
            "  (fcstart): Linear(in_features=1543, out_features=256, bias=True)\n",
            "  (fcend): Linear(in_features=64, out_features=3, bias=True)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropoutfc): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsT0tiw2Cq0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning_rate = 0.001\n",
        "# opt = optim.Adam(regressor.parameters(), lr=learning_rate)\n",
        "opt = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
        "scheduler = MultiStepLR(opt, milestones=[50,100,200,400], gamma=params[\"LRgamma\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV8n2CduHEg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Energy_mean, UVW_mean, DIR_mean = Energy.mean(dim=0).to(device), UVW.mean(dim=0).to(device), DIR.mean(dim=0).to(device)\n",
        "EZRP_mean = torch.cat([Energy_mean, UVW_mean]).to(device)\n",
        "UVWDIR_mean = torch.cat([UVW_mean, DIR_mean]).to(device)\n",
        "def metric_relative_mse(y_pred,y_true):\n",
        "    y_true_mean = y_true.mean(dim=0)\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - EZRP_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVW_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true[:,2] - y_pred[:,2]).pow(2).mean(dim=0) / (y_true[:,2] - UVW_mean[2]).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - UVWDIR_mean).pow(2).mean(dim=0)).sum()).sqrt()\n",
        "    # return (((y_true - y_pred).pow(2).mean(dim=0) / (y_true - Energy_mean).pow(2).mean(dim=0)).sum()).sqrt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue2O6r-2D1zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_fn = torch.nn.SmoothL1Loss().to(device)\n",
        "loss_fn = torch.nn.L1Loss().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmJdk_POC9Mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(epochs=100):\n",
        "    # iterating over epochs...\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        first = True\n",
        "        for Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b in train_dataloader:\n",
        "        # moving them to device(for example, cuda-device)\n",
        "            Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b = Energy_b.to(device), \\\n",
        "                                            UVW_b.to(device), \\\n",
        "                                            UVWREC_b.to(device), \\\n",
        "                                            DIR_b.to(device), \\\n",
        "                                            ADD_b.to(device), \\\n",
        "                                            PMResponse_b.to(device)\n",
        "\n",
        "#             pred = regressor(EnergyDeposit_b)\n",
        "            model.train()\n",
        "            UVWDIR_b = torch.cat([UVW_b,DIR_b],dim=1)\n",
        "            UVWDIRrec_b = torch.cat([UVWREC_b,DIR_b],dim=1)\n",
        "            # pred = model(PMResponse_b,UVWREC_b)\n",
        "            # pred = model(PMResponse_b,torch.zeros(UVW_b.shape[0],3).to(device))\n",
        "            pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "            # loss        = loss_fn(pred, torch.cat([ParticleMomentum_b, ParticlePoint_b], dim=1))\n",
        "            # loss        = loss_fn(pred, torch.cat([Energy_b,ZRP_b],dim=1))\n",
        "            loss        = loss_fn(pred, UVW_b)\n",
        "            loss_rec    = loss_fn(UVWREC_b, UVW_b)\n",
        "            # loss        = loss_fn(pred[:,2], UVW_b[:,2])\n",
        "            # loss_rec    = loss_fn(UVWREC_b[:,2], UVW_b[:,2])\n",
        "            # loss        = loss_fn(pred, Energy_b)\n",
        "\n",
        "            # model.train()\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "            train_mse       = metric_relative_mse(pred, UVW_b).item()\n",
        "            train_mse_rec   = metric_relative_mse(UVWREC_b, UVW_b).item()\n",
        "        experiment.log_metric(\"learning_rate\", scheduler.get_lr(),step=epoch)\n",
        "        experiment.log_metric(\"train_loss\", loss.item(),step=epoch)\n",
        "        experiment.log_metric(\"train_mse\", train_mse,step=epoch)\n",
        "        experiment.log_metric(\"train_loss_rec\", loss_rec.item(),step=epoch)\n",
        "        experiment.log_metric(\"train_mse_rec\", train_mse_rec,step=epoch)\n",
        "\n",
        "        if epoch%10==0:\n",
        "            plt.figure(figsize=(20,12))\n",
        "            grid = plt.GridSpec(3, 3, wspace=0.4, hspace=0.3)\n",
        "            for idim in range(3):\n",
        "                plt.subplot(grid[2,idim])\n",
        "                plt.hist2d(UVW_b[:,2].cpu().detach().numpy(),UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),\n",
        "                        range=[[-1,1],[-0.3,0.3]],\n",
        "                        bins = [40,40]\n",
        "                        )\n",
        "        scheduler.step()\n",
        "        for Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b in val_dataloader:\n",
        "    # moving them to device(for example, cuda-device)\n",
        "            Energy_b, UVW_b, UVWREC_b, DIR_b,ADD_b, PMResponse_b = Energy_b.to(device), \\\n",
        "                                            UVW_b.to(device), \\\n",
        "                                            UVWREC_b.to(device), \\\n",
        "                                            DIR_b.to(device), \\\n",
        "                                            ADD_b.to(device), \\\n",
        "                                            PMResponse_b.to(device)\n",
        "\n",
        "            break\n",
        "\n",
        "#             pred = regressor(EnergyDeposit_b)\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            UVWDIR_b = torch.cat([UVW_b,DIR_b],dim=1)\n",
        "            UVWDIRrec_b = torch.cat([UVWREC_b,DIR_b],dim=1)\n",
        "            # pred = model(PMResponse_b,UVWREC_b)\n",
        "            # pred = model(PMResponse_b,torch.zeros(UVW_b.shape[0],3).to(device))\n",
        "            pred = model(PMResponse_b,torch.cat([ADD_b,UVWREC_b],dim=1))\n",
        "            # loss        = loss_fn(pred, torch.cat([ParticleMomentum_b, ParticlePoint_b], dim=1))\n",
        "            # val_loss        = loss_fn(pred, torch.cat([Energy_b,ZRP_b],dim=1))\n",
        "            # val_loss        = loss_fn(pred, UVWDIR_b)\n",
        "            val_loss        = loss_fn(pred, UVW_b)\n",
        "            val_loss_rec    = loss_fn(UVWREC_b, UVW_b)\n",
        "            # val_loss        = loss_fn(pred[:,2], UVW_b[:,2])\n",
        "            # val_loss_rec    = loss_fn(UVWREC_b[:,2], UVW_b[:,2])\n",
        "            # val_loss        = loss_fn(pred, Energy_b)\n",
        "#             loss        = loss_fn(pred, ParticleType_b)\n",
        "            # val_mse   = metric_relative_mse(pred, torch.cat([Energy_b, ZRP_b], dim=1)).item()\n",
        "            val_mse       = metric_relative_mse(pred, UVW_b).item()\n",
        "            val_mse_rec   = metric_relative_mse(UVWREC_b, UVW_b).item()\n",
        "            # val_mse   = metric_relative_mse(pred, Energy_b).item()\n",
        "            # val_mse   = 0\n",
        "        experiment.log_metric(\"val_loss\", val_loss.item(),step=epoch)\n",
        "        experiment.log_metric(\"val_loss_rec\", val_loss_rec.item(),step=epoch)\n",
        "        experiment.log_metric(\"val_mse\", val_mse,step=epoch)\n",
        "        experiment.log_metric(\"val_mse_rec\", val_mse_rec,step=epoch)\n",
        "        resolution     =[np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy())**2)) for idim in range(3)]\n",
        "        resolution_lpf =[np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-UVWREC_b[:,idim].cpu().detach().numpy())**2)) for idim in range(3)]\n",
        "        experiment.log_metric(\"u_resolution\", resolution[0],step=epoch)\n",
        "        experiment.log_metric(\"v_resolution\", resolution[1],step=epoch)\n",
        "        experiment.log_metric(\"w_resolution\", resolution[2],step=epoch)\n",
        "        experiment.log_metric(\"u_resolution_lpf\", resolution_lpf[0],step=epoch)\n",
        "        experiment.log_metric(\"v_resolution_lpf\", resolution_lpf[1],step=epoch)\n",
        "        experiment.log_metric(\"w_resolution_lpf\", resolution_lpf[2],step=epoch)\n",
        "        if epoch%10==0:\n",
        "            # plt.figure(figsize=(20,12))\n",
        "            # grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\n",
        "            for idim in range(3):\n",
        "                plt.subplot(grid[0,idim])\n",
        "                plt.hist(UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),range=(-0.5,0.5),bins=80)\n",
        "                plt.title(\"%f\"%(np.sqrt(np.mean((UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy())**2))))\n",
        "                plt.subplot(grid[1,idim])\n",
        "                plt.hist2d(UVW_b[:,2].cpu().detach().numpy(),UVW_b[:,idim].cpu().detach().numpy()-pred[:,idim].cpu().detach().numpy(),\n",
        "                        range=[[-1,1],[-0.3,0.3]],\n",
        "                        bins = [40,40]\n",
        "                        )\n",
        "            \n",
        "            experiment.log_figure(figure=plt)\n",
        "            plt.close()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp53YVEDD3Tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "3c872662-a239-4929-b89d-e9c93ca7a78d"
      },
      "source": [
        "with experiment.train():\n",
        "    run_training(10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/10000 [00:06<17:21:10,  6.25s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 11/10000 [00:43<12:14:58,  4.41s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 21/10000 [01:21<12:05:47,  4.36s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 31/10000 [01:58<11:50:13,  4.27s/it]COMET ERROR: File could not be uploaded\n",
            "  0%|          | 41/10000 [02:36<11:58:10,  4.33s/it]COMET ERROR: File could not be uploaded\n",
            "  1%|          | 51/10000 [03:13<11:55:33,  4.32s/it]COMET ERROR: File could not be uploaded\n",
            "  3%|         | 260/10000 [16:48<9:51:01,  3.64s/it] "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv8G78FLfZd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}